<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Generating Long-Term Structure in Songs and Stories</title>
  <meta name="description" content="One of the difficult problems in using machine learning to generate sequences, such as melodies, is creating long-term structure. Long-term structure comes v...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/">
  <link rel="alternate" type="application/rss+xml" title="Magenta" href="https://magenta.tensorflow.org/feed.xml">
  <script src="//www.google.com/js/gweb/analytics/autotrack.js"></script>
  <script>
  new gweb.analytics.AutoTrack({
    profile: 'UA-80107903-1'
  });
  </script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <a class="site-title" href="/"><span class="site-title"><img src="/assets/magenta-logo.png" height=75></span>
</a -->
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="https://www.github.com/tensorflow/magenta">GitHub</a>
        <a class="page-link" href="/feed.xml">RSS feed</a>
        <a class="page-link" href="https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss">Discuss</a>
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        


      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Generating Long-Term Structure in Songs and Stories</h1>

    <p class="post-meta"><time datetime="2016-07-15T17:00:00+00:00" itemprop="datePublished">Jul 15, 2016</time>
     •
      
        Elliot Waite
      
      
        (<a href="https://github.com/elliotwaite"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">elliotwaite</span></a>
)
      
    </span>
    </span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>One of the difficult problems in using machine learning to generate sequences, such as melodies, is creating long-term structure. Long-term structure comes very naturally to people, but it’s very hard for machines. Basic machine learning systems can generate a short melody that stays in key, but they have trouble generating a longer melody that follows a chord progression, or follows a multi-bar song structure of verses and choruses. Likewise, they can produce a screenplay with grammatically correct sentences, but not one with a compelling plot line. Without long-term structure, the content produced by recurrent neural networks (RNNs) often seems wandering and random.</p>

<p>But what if these RNN models could recognize and reproduce longer-term structure?<!--more--> Could they produce content that feels more meaningful – more human? Today we’re open-sourcing two new Magenta models, <b><a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/lookback_rnn" target="_blank">Lookback RNN</a></b> and <b><a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/attention_rnn" target="_blank">Attention RNN</a></b>, both of which aim to improve RNNs’ ability to learn longer-term structures. We hope you’ll join us in exploring how they might produce better songs and stories.</p>

<h1 id="lookback-rnn">Lookback RNN</h1>

<p>Lookback RNN introduces custom inputs and labels. The custom inputs allow the model to more easily recognize patterns that occur across 1 and 2 bars. They also help the model recognize patterns related to where in the measure an event occurs. The custom labels make it easier for the model to repeat sequences of notes without having to store them in the RNN’s cell state. The type of RNN cell used in this model is an LSTM.</p>

<p>In our introductory model, Basic RNN, the input to the model was a one-hot vector of the previous event, and the label was the target next event. The possible events were note-off (turn off any currently playing note), no event (if a note is playing, continue sustaining it, otherwise continue silence), and a note-on event for each  pitch (which also turns off any other note that might be playing). In Lookback RNN, we add the following additional information to the input vector:</p>

<ul>
  <li>
    <p>In addition to inputting the previous event, we also input the events from 1 and 2 bars ago. This allows the model to more easily recognize patterns that occur across 1 and 2 bars, such as mirrored or contrasting melodies.</p>
  </li>
  <li>
    <p>We also input whether the last event was repeating the event from 1 or 2 bars before it. This signals if the last event was creating something new, or just repeating an already established melody. This allows the model to more easily recognize patterns associated with being in a repetitive or non-repetitive state.</p>
  </li>
  <li>
    <p>We also input the current position within the measure (as done previously by <a href="http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/" target="_blank">Daniel Johnson</a>), allowing the model to more easily learn patterns associated with 4/4 time music. These inputs are 5 values that can be thought of as a binary step clock.<br />
Step 1: <script type="math/tex">[0, 0, 0, 0, 1]</script><br />
Step 2: <script type="math/tex">[0, 0, 0, 1, 0]</script><br />
Step 3: <script type="math/tex">[0, 0, 0, 1, 1]</script><br />
Step 4: <script type="math/tex">[0, 0, 1, 0, 0]</script><br />
The only difference being the values are -1 and 1 instead of 0 and 1.</p>
  </li>
</ul>

<p>In addition to feeding the model more input information, we also add two new custom labels. The label to repeat the event from 1 bar ago and the label to repeat the event from 2 bars ago. This is where the Lookback RNN gets its name. When creating labels for the training data, if the current event in the melody is repeating the same event from 2 bars ago, we set the label for that step to be repeat-2-bars-ago. If it’s not repeating the event from 2 bars ago, we check if it’s repeating the event from 1 bar ago, and if so, we set the label for that step to be repeat-1-bar-ago. Only when the melody isn’t repeating 1 or 2 bars ago do we make the label for that step be a specific melody event. For example, if the third bar of the melody is completely repeating the first bar, every label for that third bar will be the repeat-2-bars-ago label. This allows the model to more easily repeat 1 or 2 bar phrases without having to store those sequences in its memory cell. Since a lot of melodies in popular music repeat events from 1 and 2 bars ago, these extra labels reduce the complexity of information the model has to learn to represent.</p>

<p>Here are some sample melodies generated by the Lookback RNN model when trained on a collection of popular music. The intro notes (played on the glockenspiel) were given to the model as a priming melody. The rest of the notes were generated.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/vkdQEO0UBnY" frameborder="0" allowfullscreen=""></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5BvzdkRvMmI" frameborder="0" allowfullscreen=""></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/JA_DkaIYAoE" frameborder="0" allowfullscreen=""></iframe>
<p><br />
To train the Lookback RNN on your own MIDI collection and generate your own melodies from it, follow the steps in the <a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/lookback_rnn" target="_blank">README</a></p>

<h1 id="attention-rnn">Attention RNN</h1>

<p>To learn even longer-term structure we can use attention. Attention is one of the ways that models can access previous information without having to store it in the RNN cell’s state. The RNN cell used in this model is an LSTM. The attention method used comes from the paper <a href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (D Bahdanau, K Cho, Y Bengio, 2014). In that paper, the model is an encoder-decoder RNN, and the model uses attention to look at all the encoder outputs during each decoder step. In our version, where we don’t have an encoder-decoder, we just always look at the outputs from the last <script type="math/tex">n</script> steps when generating the output for the current step. The way we “look at” these steps is with an attention mechanism. Specifically:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
u_i^t &= v^T \text{tanh}(W_1^\prime h_i + W_2^\prime c_t) \\
a_i^t &= \text{softmax}(u_i^t) \\
h_t^\prime &= \sum_{i=t-n}^{t-1} a_i^t h_i
\end{align} %]]></script>

<p>The vector <script type="math/tex">v</script> and matrices <script type="math/tex">W_1^\prime</script>, <script type="math/tex">W_2^\prime</script> are learnable parameters of the model. <script type="math/tex">h_i</script> are the RNN outputs from the previous <script type="math/tex">n</script> steps <script type="math/tex">(h_{t-n},...,h_{t-1})</script>, and vector <script type="math/tex">c_t</script> is the current step’s RNN cell state. These values are used to calculate <script type="math/tex">u_i^t</script> <script type="math/tex">(u_{t-n}^t,...,u_{t-1}^t)</script>, an <script type="math/tex">n</script> length vector with one value for each of the previous <script type="math/tex">n</script> steps. The values represent how much attention each step should receive. A softmax is used to normalize these values and create a mask-like vector <script type="math/tex">a_i^t</script>, called the attention mask. The RNN outputs from the previous <script type="math/tex">n</script> steps are then multiplied by these attention mask values and then summed together to get <script type="math/tex">h_t^\prime</script>. For example, let’s assume we are on the 4th step of our sequence and <script type="math/tex">n</script> = 3, which means our attention mechanism is only looking at the last 3 steps. For this example, the RNN output vectors will be small 4 length vectors. If the RNN outputs from the first 3 steps are:</p>

<p>Step 1: <script type="math/tex">[1.0, 0.0, 0.0, 1.0]</script><br />
Step 2: <script type="math/tex">[0.0, 1.0, 0.0, 1.0]</script><br />
Step 3: <script type="math/tex">[0.0, 0.0, 0.5, 0.0]</script></p>

<p>And our calculated attention mask is:</p>

<p><script type="math/tex">a_{i}^{t}</script> <script type="math/tex">= [0.7, 0.1, 0.2]</script></p>

<p>Then the previous step would get 20% attention, 2 steps ago would get 10% attention, and 3 steps ago would get 70% attention. So their masked values would be:</p>

<p>Step 1 (70%): <script type="math/tex">[0.7, 0.0, 0.0, 0.7]</script><br />
Step 2 (10%): <script type="math/tex">[0.0, 0.1, 0.0, 0.1]</script><br />
Step 3 (20%): <script type="math/tex">[0.0, 0.0, 0.1, 0.0]</script></p>

<p>And then they’d be summed together to get <script type="math/tex">h_t^\prime</script>:</p>

<p><script type="math/tex">h_t^\prime</script> <script type="math/tex">= [0.7, 0.1, 0.1, 0.8]</script></p>

<p>The <script type="math/tex">h_t^\prime</script> vector is essentially all <script type="math/tex">n</script> previous outputs combined together, but each output contributing a different amount relative to how much attention that step received.</p>

<p>This <script type="math/tex">h_t^\prime</script> vector is then concatenated with the RNN output from the current step and a linear layer is applied to that concatenated vector to create the new output for the current step. Some attention models only apply this <script type="math/tex">h_t^\prime</script> vector to the RNN output, but in our model, as is also sometimes done, this <script type="math/tex">h_t^\prime</script> vector is also applied to the input of the next step. The <script type="math/tex">h_t^\prime</script> vector is concatenated with the next step’s input vector and a linear layer is applied to that concatenated vector to create the new input to the RNN cell. This helps attention not only affect the data coming out of the RNN cell, but also the data being fed into the RNN cell.</p>

<p>This <script type="math/tex">h_t^\prime</script> vector, which is a combination of the outputs from the previous <script type="math/tex">n</script> steps, is how attention can directly inject information from those previous steps into the current step’s network of calculations, making it easier for the model to learn longer-term dependencies without having to store all that information from those previous steps in the RNN cell’s state. If you’d like an even deeper understanding of the whole attention process, you can walk through <a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/attention_rnn/attention_rnn_graph.py" target="_blank">the code</a> to see exactly what’s happening.</p>

<p>Here are some sample melodies generated by the Attention RNN model when trained on a collection of popular music. These melodies were all primed with the first four notes of Twinkle Twinkle Little Star, then the rest of the notes were generated by the model:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/nHxr9u9_4_s" frameborder="0" allowfullscreen=""></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/iZ-OqPzOqIQ" frameborder="0" allowfullscreen=""></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yW-SvX64xOA" frameborder="0" allowfullscreen=""></iframe>
<p><br />
Melody 1 and 2 were combined in a standard song format, AABA, and backed up by drums to create the following song sample:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/TgKd8_r-yl8" frameborder="0" allowfullscreen=""></iframe>
<p><br />
Jason Nguyen (<a href="https://www.youtube.com/user/SoulGook/" target="_blank">@SoulGook</a>), on the đàn bầu, and Alex Koman (<a href="https://www.facebook.com/meloscribe" target="_blank">@meloscribe</a>), on guitar, added to that song to create this man and machine collaboration:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Aq3370Prbi4" frameborder="0" allowfullscreen=""></iframe>
<p><br />
The following song uses the three Attention RNN melodies listed above by layering them all together. They compliment each other surprisingly well. The drums and bass line were added by a human. This demonstrates how musicians could use these generated melodies for building out larger, more elaborate songs.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/qFBQDfPyjoE" frameborder="0" allowfullscreen=""></iframe>
<p><br />
To train the Attention RNN on your own MIDI collection and generate your own melodies from it, follow the steps in the <a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/attention_rnn" target="_blank">README</a> on GitHub.</p>

<p>These models improve on the initial Magenta <a href="https://github.com/tensorflow/magenta/blob/master/magenta/models/basic_rnn" target="_blank">Basic RNN</a> by adding two forms of memory manipulation, simple lookback and learned attention. Nevertheless, a lot of work remains before Magenta models are writing complete pieces of music or telling long stories. Stay tuned for more improvements.</p>

<p>Edit (@elliotwaite Aug 8, 2016): Updated the reference for the attention method used to the paper that originally introduced the idea, <a href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (D Bahdanau, K Cho, Y Bengio, 2014).</p>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  </div>

</article>

      </div>
    </div>

  </body>

</html>
