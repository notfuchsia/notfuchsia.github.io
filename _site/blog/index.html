<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Blog</title>
  <meta name="description" content="Testing blog for editing.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/">
  <link rel="alternate" type="application/rss+xml" title="Not Fuchsia" href="http://localhost:4000/feed.xml">
  <script src="//www.google.com/js/gweb/analytics/autotrack.js"></script>
  <script>
  new gweb.analytics.AutoTrack({
    profile: 'UA-80107903-1'
  });
  </script>
</head>


  <body>

    <header class="site-header">
	<div class="top-thingy">
	  <div class="wrapper">
			 <a class="site-title" href="/"><span class="site-title"><img src="/assets/magenta-logo.png" height=75></span>
</a -->
	     <nav class="site-nav">
	         <a href="#" class="menu-icon">
	           <svg viewBox="0 0 18 15">
	             <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
	             <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
	             <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
	           </svg>
	         </a>
			 <div class="trigger">
				 <a class="page-link" href="/">About</a> 
				 <a class="page-link" href="/blog/">Blog</a> 				 
				 <a class="page-link" href="https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss">Community</a>
				 <a class="page-link" href="https://www.github.com/tensorflow/magenta">GitHub</a>
			 </div>
	    </nav>
	</div>
	</div>
	</div>
	<div class="hero-thin" style="background-image: url(/assets/hero-thin.png);"></div>
</header>

    <div class="page-content">
      <div class="wrapper">
        <div class="home">
  <ul class="post-list">
    
      
      <li>
        <h2>
          <a class="post-link" href="/2016/10/24/natasha/">Tuning Recurrent Neural Networks with Reinforcement Learning</a>
        </h2>
        <span class="post-meta">Oct 24, 2016</span>
        •
         
           Natasha Jaques
         
         
           (<a href="https://github.com/natashamjaques"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">natashamjaques</span></a>
)
         
        
        <p>We are excited to announce our new <a href="https://arxiv.org/abs/comingsoon">RL Tuner algorithm</a>, a method for enchancing the performance of an LSTM trained on data using Reinforcement Learning (RL). We create an RL reward function that teaches the model to follow certain rules, while still allowing it to retain information learned from data. We use <em>RL Tuner</em> to teach concepts of music theory to an LSTM trained to generate melodies. The two videos below show samples from the original LSTM model, and the same model enchanced using <em>RL Tuner</em>.</p>

<div style="text-align:center">
	<iframe style="display: inline-block;margin-right:5px" width="280" height="280" src="https://www.youtube.com/embed/cDcsOokicLw" frameborder="0" allowfullscreen=""></iframe><iframe style="display: inline-block;" width="280" height="280" src="https://www.youtube.com/embed/abBfZB5DlSY" frameborder="0" allowfullscreen=""></iframe>
</div>
<p><br /></p>

<h2 id="introduction">Introduction</h2>

<p><img style="margin-right:10px" align="left" src="/assets/rl_tuner/note_rnn_model.png" width="200" /></p>

<p>When I joined <a href="https://magenta.tensorflow.org/">Magenta</a> as an intern this summer, the team was hard at work on developing better ways to train Recurrent Neural Networks (RNNs) to generate sequences of notes. As you may remember from <a href="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/">previous posts</a>, these models typically consist of a Long Short-Term Memory (LSTM) network trained on monophonic melodies. This means that melodies are fed into the network one note at a time, and it is trained to predict the next note in the sequence. The figure on the left shows a simplified version of this type of network unrolled over time, in which it is being trained on the first 6 notes of “Twinkle Twinkle Little Star”. From now on, I’ll refer to this type of model as a <strong>Note RNN</strong>.</p>

<!--With a little hard work and creativity, a *Note RNN* can produce [nice melodies][elliot song].-->
<p>A <em>Note RNN</em> is conceptually similar to a <em>Character RNN</em>, a popular model for generating text, one character at a time. While both types of models can produce impressive results, they have some frustrating limitations. They both suffer from common failure modes, such as continually repeating the same token. Further, the sequences produced by the models tend to lack a consistent global structure. To see this more clearly, take a look at the text below, which was generated by a Character RNN trained on Wikipedia markdown data (taken from <a href="https://arxiv.org/pdf/1308.0850.pdf">Graves, 2013</a>):</p>

<blockquote>
  <p>The ‘'’Rebellion’’’ (‘‘Hyerodent’’) is [[literal]], related mildly older than old half sister, the music, and morrow been much more propellent. All those of [[Hamas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|“Sante” at Serassim]]; “Verra” as 1865-682-831 is related to ballistic missiles. While she viewed it friend of Halla equatorial weapons of Tuscany, in [[France]], from vaccine homes to "individual", among [[slavery|slaves]](such as artistual selling of factories were renamed English habit of twelve years.)</p>
</blockquote>

<p>While the model has learned how to correctly spell English words, some markdown syntax, and even mostly correct grammatical structure, sentences don’t seem to follow a consistent thought. The text wonders rapidly from topic to topic, discussing everything from sisters to submarines to Tuscany, slavery, and factories, all in one short paragraph. This type of global incoherence is typical of the melodies produced by a vanilla <em>Note RNN</em> as well. They do not maintain a consistent musical structure, and can sound wandering and random.</p>

<p>Music is an interesting test-bed for sequence generation, in that musical compositions adhere to a relatively well-defined set of structural rules. Any beginning music student learns that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases. So what if we could teach a <em>Note RNN</em> these kinds of musical rules, while still allowing it to learn patterns from music it hears in the world?</p>

<p>This is the idea behind the <strong>RL Tuner</strong> model I will describe in this post. We take a trained <em>Note RNN</em> and teach it concepts of music theory using Reinforcement Learning (RL). RL can allow a network to learn some non-differentiable <em>reward</em> function. In this case, we define a set of music theory rules, and produce rewards based on whether the model’s compositions adhere to those rules. However, to ensure that the model can remember the note probabilities it originally learned from data, we keep a second, fixed copy of the <em>Note RNN</em> which we call the <em>Reward RNN</em>. The <em>Reward RNN</em> is used to compute the probability of playing the next note as learned by the original <em>Note RNN</em>. We augment our music theory rewards with this probability value, so that the total reward reflects both our music theory constraints and information learned from data.</p>

<p>We show that this approach allows the model to maintain information about the note probabilities learned from data, while significantly improving the behaviors of the <em>Note RNN</em> targeted by the music theory rewards. For example, before training with RL, 63.3% of notes produced by the <em>Note RNN</em> belonged to some excessively repeated segment of notes; after RL, 0.0-0.03% of notes were excessively repeated. Since excessively repeating tokens is a problem in other domains as well (e.g. text generation), we believe our approach could have broader applications. But does it actually work to produce better music? We conducted a user study and found that people find the compositions produced by our three RL models significantly more musically pleasing than those of the original <em>Note RNN</em>. But we encourage you to judge for yourself; samples from each of the models will be provided later in this post.</p>

<p>The models, derivations, and results are all described in our recent <a href="https://arxiv.org/abs/comingsoon">research paper</a>, written by myself, <a href="http://sg717.user.srcf.net/">Shane Gu</a>, <a href="http://learning.eng.cam.ac.uk/Public/Turner/WebHome">Richard E. Turner</a>, and Douglas Eck<sup><a href="#footnote-nipsrl">1</a></sup>. <a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/rl-tuner">Code</a> to run this model is also available on the Magenta github repo; please try it out! The music theory rules implemented for the model are only a first attempt, and could easily be improved by someone with musical training.</p>

<p><a name="footnote-nipsrl"><sub><sup>1</sup></sub></a> <sub><sup>A version of this work was accepted at the <a href="https://sites.google.com/site/deeprlnips2016/">NIPS 2016 Deep Reinforcement Learning Workshop</a>.</sup></sub></p>

<h2 id="background-reinforcement-learning-and-deep-q-learning">Background: Reinforcement Learning and Deep Q-Learning</h2>
<p><em>This section will give a brief introduction to some ideas behind RL and Deep Q Networks (DQNs). If you’re familiar with these topics you may wish to skip ahead.</em></p>

<p>In reinforcement learning (RL), an agent interacts with an environment. Given the state of the enviornment <script type="math/tex">s</script>, the agent takes an action <script type="math/tex">a</script>, receives a reward <script type="math/tex">r</script>, and the environment transitions to a new state, <script type="math/tex">s'</script>. The goal of the agent is to maximize reward, which is usually some clear signal from the environment, such as points in a game.</p>

<p>The rules for how the agent chooses to act in the environment define a <em>policy</em>. To learn the most effective policy, the agent can’t just greedily maximize the reward it will receive after the next action, but must instead consider the total cumulative reward it can expect to receive over a course of actions occurring in the future. Because future rewards are typically uncertain if the environment has random effects, a discount factor of <script type="math/tex">\gamma</script> is applied to the reward for each timestep in the future. If <script type="math/tex">r_t</script> is the reward received at timestep <script type="math/tex">t</script>, then <script type="math/tex">R_t</script> is the total future discounted <em>return</em>:
\begin{align}
R_t = \sum^T_{t’=t}\gamma^{t’-t}r_{t’} 
\end{align}</p>

<p>In Q-learning, the goal is to learn a Q function that gives the maximum expected discounted future return for taking any action <script type="math/tex">a</script> in state <script type="math/tex">s</script>, and continuing to act optimally at each step in the future. Therefore the optimal Q function, <script type="math/tex">Q^*</script> is defined as:</p>

<script type="math/tex; mode=display">\begin{align}
Q^*(s, a) = max_\pi \mathbb{E}[R_t|s_t = s, a_t = a, \pi]
\end{align}</script>

<p>where <script type="math/tex">\pi</script> is the policy mapping each state to a probability distributions over the next action. To learn <script type="math/tex">Q*</script>, we can apply an iterative update based on the Bellman equation:
\begin{align}
Q_{i+1}(s, a) = \mathbb{E}[r + \gamma max_{a’}Q_i(s’,a’)|s,a]
\end{align}
where <em>r</em> is the reward received for taking action <em>a</em> in state <em>s</em>.  This value iteration method will converge to <script type="math/tex">Q^*</script> as <script type="math/tex">i \rightarrow \infty</script>. While learning, it is important to explore the space of possible actions, either by occasionally choosing random actions, or sampling an action based on the values defined by the <script type="math/tex">Q</script> function. Once the <script type="math/tex">Q</script> function has been learned, the optimal policy can be obtained by simply choosing the action with the highest <script type="math/tex">Q</script> value at every step.</p>

<p>In <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Deep Q Learning</a>, a neural network called the Deep Q-network (DQN) is used to approximate the <script type="math/tex">Q</script> function, <script type="math/tex">Q(s, a; \theta) \approx Q^*(s, a)</script>. The network parameters <script type="math/tex">\theta</script> are learned by applying stochastic gradient descent (SGD) updates with respect to the following loss function:
\begin{align} 
\label{eq:qloss}
L_t(\theta_t) = (r_t + \gamma \max_{a’}Q(s’,a’;\theta_{t-1}) - Q(s,a;\theta_t))^2
\end{align}
The first two terms are the <script type="math/tex">Q</script> function the network is trying to learn: the actual reward received at step <script type="math/tex">t</script>, <script type="math/tex">r_t</script>, plus the discounted future return estimated by the <em>Q-network</em> parameters at step <script type="math/tex">t-1</script>. The loss function computes the difference between this desired <script type="math/tex">Q</script> value, and the actual value output by the <em>Q-network</em> at step <script type="math/tex">t</script>. Essentially, this is the prediction error in estimating the <script type="math/tex">Q</script> function made by the <em>Q-network</em>. Importantly, the parameters for the previous generation of the network (<script type="math/tex">\theta_{t-1}</script>) are held fixed, and not updated by SGD.</p>

<!--While interacting with the environment, the $$Q$$-learning agent typically follows an $$\epsilon$$-greedy policy. This means that it will greedily choose the action with the highest Q value with probability $$1-\epsilon$$, and choose a random action with probability $$\epsilon$$. This off-policy learning method ensures a reasonably trade-off between exploration of the action space, and exploitation of the most efficient strategies learned so far. -->

<p>Several techniques are required for a DQN to work effectively. As the agent interacts with the environment, <script type="math/tex">% <![CDATA[
<s_t,a_t,r_t,s_{t+1}> %]]></script> tuples it experiences are stored in an <em>experience buffer</em>. Training the <em>Q-network</em> is accomplished by randomly sampling batches from the <em>experience buffer</em> to compute the loss. The <em>experience buffer</em> is essential for learning; if the agent was trained using consecutive samples of experience, the samples would be highly correlated, updates would have high variance, and the network parameters could become stuck in a local minimum or diverge.</p>

<p>Further optimizations to the DQN algorithm have been proposed that help enhance learning and ensure stability. One of these is <a href="http://www.aaai.org/Conferences/AAAI/2016/Papers/12vanHasselt12389.pdf">Deep Double Q-learning</a>, in which a second, <em>Target Q-network</em> is used to estimate expected future return, while the <em>Q-network</em> is used to choose the next action. Since Q-learning has been shown to learn unrealistically high action values because it estimates maximum expected return, having a second Q-network can lead to more realistic estimates and better performance.</p>

<h2 id="rl-tuner">RL Tuner</h2>
<p>As described above, the main idea behind the RL Tuner model is to take an RNN trained on data, and refine it using RL. The model uses a standard DQN implementation, complete with an <em>experience buffer</em> and <em>Target Q-network</em>. A trained <em>Note RNN</em> is used to supply the initial values of the weights in the <em>Q-network</em> and <em>Target Q-network</em>, and a third copy is  used as the <em>Reward RNN</em>. The <em>Reward RNN</em> is held fixed during training, and is used to supply part of the reward function used to train the model. The figure below illustrates these ideas.</p>

<p><img src="/assets/rl_tuner/rl_rnn_diagram.png" alt="Model diagram" title="Model diagram" /></p>

<p>To formulate musical composition as an RL problem, we treat choosing the next note as taking an action <script type="math/tex">a</script>. The state of the environment <script type="math/tex">s</script> consists of the state of the composition so far, as well as the internal LSTM state of the <em>Q-network</em> and <em>Reward RNN</em>. The reward function is a combination of both music theory rules and probabilities learned from data. The music theory reward <script type="math/tex">r_{MT}(a,s)</script> is calculated by a set of functions (described in the next section) that constrain the model to adhere to certain rules, such as playing in the same key. However, it is necessary that the model still be “creative” rather than learning a simple composition that can easily exploit these rewards. Therefore, the <em>Reward RNN</em> is used to compute <script type="math/tex">p(a</script>|<script type="math/tex">s)</script>, the probability of playing the next note <script type="math/tex">a</script> given the composition <script type="math/tex">s</script> as originally learned from actual songs. The total reward given at time <script type="math/tex">t</script> is therefore:</p>

<script type="math/tex; mode=display">\begin{align} 
\label{eq:reward}
r_t(a,s) = \log p(a|s) + \frac{1}{c}r_{MT}(a,s)
\end{align}</script>

<p>where <script type="math/tex">c</script> is a constant controlling the emphasis placed on the music theory reward. So now, we see that the new loss function for our model is:</p>

<script type="math/tex; mode=display">\begin{align}
\label{eq:melody_dqn_loss}
L_t(\theta_t) = (\log p(a|s) + \frac{1}{c}r_{MT}(a,s) + \gamma \max_{a'}Q(s',a';\theta_{t-1}) - Q(s,a;\theta_t))^2
\end{align}</script>

<p>This modified loss function forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have the high probability in the original data.</p>

<h3 id="for-the-mathematically-inclined">For the mathematically inclined…</h3>

<p>In <a href="https://arxiv.org/abs/comingsoon">our paper</a> we show that the loss function described above can be related to an approximation of the Stochastic Optimal Control objective, leading to a RL cost with an additional penalty applied to KL-divergence from a prior policy. If we think of the probabilities learned by the <em>Note RNN</em> as the prior policy <script type="math/tex">p(\cdot</script>|<script type="math/tex">s)</script>, and <script type="math/tex">\pi_{\theta}</script> as the policy learned by the model, then this would be equivalent to learning the following function:</p>

<script type="math/tex; mode=display">\begin{align} 
\mathbb{E}_{\pi}[\sum_t r(s_t,a_t)/c - KL[\pi_\theta(\cdot|s_t)||p(\cdot|s_t)]]
\end{align}</script>

<p>This is not exactly the same as our method, because we are missing the entropy term in the KL-divergence function. This led us to implement two other KL-regularized variants of Q-learning: <a href="http://homepages.inf.ed.ac.uk/svijayak/publications/rawlik-RSS2012.pdf"><script type="math/tex">\Psi</script> learning</a>, and <a href="https://arxiv.org/pdf/1512.08562.pdf">G learning</a>. The loss function for <script type="math/tex">\Psi</script> learning is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
&L_{\Psi}(\theta)= \mathbb{E}[(\log p(a|s) + \frac{1}{c}r_{MT}(s,a) + \gamma \log \sum_{a'} e^{\Psi(s',a';\theta^-)} - \Psi(s,a;\theta))^2]
\end{align} %]]></script>

<p>And the loss function for G learning is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
&L_G(\theta)= \mathbb{E}_\beta[(\frac{1}{c}r_{MT}(s,a)+ \gamma \log \sum_{a'} e^{\log p(a'|s')+G(s',a';\theta^-)} - G(s,a;\theta))^2]
\end{align} %]]></script>

<h2 id="music-theory-rewards">Music Theory Rewards</h2>
<p>The rules we implemented to make sure our model conformed to music theory were based on some domain knowledge, and the book <a href="http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents">“A Practical Approach to Eighteenth-Century Counterpoint” by Robert Gauldin</a>. We are by no means claiming that these rules are necessary for good compositions, exhaustive, or even particularly creative. They simply allow us to constrain our model to adhere to some sort of consistent structure. We encourage any interested readers to experiment with different rules and see what types of results they can produce. For now, the rules that we chose encourage the compositions produced by our model to have the following characteristics:</p>

<ul>
  <li>
    <p><strong>Stay in key</strong>: Notes should belong to the same key. For example, if the desired key is C-major, a B-flat would not be an acceptable note.</p>
  </li>
  <li>
    <p><strong>Begin and end with the tonic note</strong>: The first note of the composition, and the first note of the final bar should be the tonic note of the key; e.g. if the key is C-major, this note would be middle C.</p>
  </li>
  <li>
    <p><strong>Avoid excessively repeated notes</strong>: Unless a rest is introduced or a note is held, a single tone should not be repeated more than four times in a row. While the number four can be considered a rough heuristic, avoiding excessively repeated notes and static melodic contours is <a href="http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents">Gauldin’s</a> first rule of melodic composition.</p>
  </li>
  <li>
    <p><strong>Prefer harmonious intervals</strong>: The composition should avoid awkward intervals like augmented sevenths, or large jumps of more than an octave. Gauldin also indicates good compositions should move by a mixture of small steps and larger harmonic intervals, with emphasis on the former; the reward values for intervals reflect these requirements.</p>
  </li>
  <li>
    <p><strong>Resolve large leaps</strong>: When the composition leaps several pitches in one direction, it should eventually be resolved by a leap back or gradual movement in the opposite direction. Leaping twice in the same direction is negatively rewarded.</p>
  </li>
  <li>
    <p><strong>Avoid continuously repeating extrema notes</strong>: The highest note of the composition should be unique, as should the lowest note.</p>
  </li>
  <li>
    <p><strong>Avoid high auto-correlation</strong>: To encourage variety, the reward function penalizes a melody if it is highly correlated with itself at a lag of 1, 2, or 3 beats.</p>
  </li>
  <li>
    <p><strong>Play motifs</strong>: A musical motif is a succession of notes representing the shortest musical “idea”; in our implementation, it is defined as a bar of music with three or more unique notes.</p>
  </li>
  <li>
    <p><strong>Play repeated motifs</strong>: Because <a href="https://secureweb.mcgill.ca/spl/files/spl/livingstoneemotion2012.pdf">repetition has been shown to be key to emotional engagement with music</a>, we tried to train the model to repeat motifs that it had previously introduced.</p>
  </li>
</ul>

<h2 id="results">Results</h2>
<p>To see if our models actually learned what we were trying to teach them, we computed statistics about how many of the notes and compositions generated by the models adhered to our music theory rules. The results are shown in the table below, and represent statistics about 100,000 compositions randomly generated by each model. The top section shows behaviors we want to decrease, and the bottom show behaviors we want to increase. Bolded entries are significantly better than the basic <em>Note RNN</em>.</p>

<table style="width:100%">
	<thead style="border-bottom:1px">
		<tr>
			<th style="text-align:left"> Behavior   </th>
			<th style="text-align:left"> Note RNN </th>
			<th style="text-align:left"> Q   </th>
			<th style="text-align:left"> Psi </th>
			<th style="text-align:left"> G </th>
		</tr>
	</thead>
	<tr>
		<td colspan="5" style="border-bottom:1px solid black;"></td>
	</tr>
	<tr>
		<td> Notes excessively repeated   </td>
		<td> 63.3% </td>
		<td> <strong>0.0%</strong> </td>
		<td> <strong>0.02%</strong> </td>
		<td> <strong>0.03%</strong> </td>
	</tr>
	<tr>
		<td> Notes not in key   </td>
		<td> 0.1% </td>
		<td> 1.0% </td>
		<td> 0.6% </td>
		<td> 28.7% </td>
	</tr>
	<tr>
		<td> Mean autocorrelation (lag 1,2,3)   </td>
		<td> -.16, .14, -.13 </td>
		<td> <strong>-.11, .03, .03</strong> </td>
		<td> <strong>-.10, -.01, .01</strong> </td>
		<td> .55, .31, .17 </td>
	</tr>
	<tr>
		<td colspan="5" style="border-bottom:1px solid black;"></td>
	</tr>
	<tr>
		<td> Leaps resolved   </td>
		<td> 77.2% </td>
		<td> <strong>91.1%</strong> </td>
		<td> <strong>90.0%</strong> </td>
		<td> 52.2% </td>
	</tr>
	<tr>
		<td> Compositions starting with tonic   </td>
		<td> 0.9% </td>
		<td> <strong>28.8%</strong> </td>
		<td> <strong>28.7%</strong> </td>
		<td> 0.0% </td>
	</tr>
	<tr>
		<td> Compositions with unique max note   </td>
		<td> 64.7% </td>
		<td> 56.4% </td>
		<td> 59.4% </td>
		<td> 37.1% </td>
	</tr>
		<tr>
		<td> Compositions with unique min note   </td>
		<td> 49.4% </td>
		<td> 51.9% </td>
		<td> <strong>58.3%</strong> </td>
		<td> <strong>56.5%</strong> </td>
	</tr>
	<tr>
		<td> Notes in motif   </td>
		<td> 5.9% </td>
		<td> <strong>75.7%</strong> </td>
		<td> <strong>73.8%</strong> </td>
		<td> <strong>69.3%</strong> </td>
	</tr>
	<tr>
		<td> Notes in repeated motif   </td>
		<td> 0.007% </td>
		<td> <strong>0.11%</strong> </td>
		<td> <strong>0.09%</strong> </td>
		<td> <strong>0.01%</strong> </td>
	</tr>
</table>

<p><br /><br />
These results show that the targeted behaviors have significantly improved in the RL models as compared to the original <em>Note RNN</em>. But how do we know that learning these rules didn’t cause the models to forget what they learned from data? The figures below plot the rewards received by each model over time, broken down into the <script type="math/tex">\log p(a</script>|<script type="math/tex">s)</script> rewards from the Note RNN (on the left), and the music theory rewards <script type="math/tex">r_{MT}</script> (on the right). Each model is compared to a baseline <em>RL only</em> model that was trained using only the music theory rewards, and no information about the data probabilities. We see that compared to the <em>RL only</em> model, the RL Tuner models maintain much higher <script type="math/tex">\log p(a</script>|<script type="math/tex">s)</script>, while still learning the music theory rewards.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/rl_tuner/rewards_note_rnn.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/rl_tuner/rewards_music_theory.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Note RNN rewards</td>
      <td style="text-align: center">Music theory rewards</td>
    </tr>
  </tbody>
</table>

<p><br />
Even so, it’s not clear that learning this somewhat arbitrary set of music theory rules will lead to better sounding compositions. To find out whether the models have improved over the original <em>Note RNN</em>, we asked Mechanical Turk workers to rate which of two randomly selected compositions they preferred. The figure below shows the number of times a composition from each model was selected as the winner. All three RL Tuner models significantly outperformed the <em>Note RNN</em>, <script type="math/tex">% <![CDATA[
p<.001 %]]></script>. In addition, both the <script type="math/tex">\Psi</script> and <script type="math/tex">Q</script> models were statistically significantly better than the <script type="math/tex">G</script> model, although not significantly different from each other.</p>

<div style="text-align:center"><img src="/assets/rl_tuner/user_study.png" width="400" /></div>

<p><br />
But you don’t need to take our word for it! The compositions used in the study are all available <a href="goo.gl/XIYt9m">here</a> - please check them out! Below, you can listen to a composition from each model. In the top row are compositions from the <em>Note RNN</em> and <script type="math/tex">G</script>; in the bottom row are <script type="math/tex">\Psi</script> and <script type="math/tex">Q</script>. You can see that the <em>Note RNN</em> plays the same note repeatedly, while the <em>RL Tuner</em> models sound much more varied and interesting. The <script type="math/tex">\Psi</script> and <script type="math/tex">Q</script> are best at playing within the music theory contraints, staying firmly in key and frequently choosing more harmonious interval steps. Still, we can tell that the models have retained information about the training songs. The <script type="math/tex">Q</script> sample ends with a riff that sounds very familiar!</p>

<div style="text-align:center">
	<iframe style="display: inline-block;margin-right:5px" width="280" height="280" src="https://www.youtube.com/embed/c5wm9NR1QiU" frameborder="0" allowfullscreen=""></iframe><iframe style="display: inline-block;" width="280" height="280" src="https://www.youtube.com/embed/HjCxS53Ta14" frameborder="0" allowfullscreen=""></iframe>
	<iframe style="display: inline-block;margin-right:5px" width="280" height="280" src="https://www.youtube.com/embed/PavKsrzFAEI" frameborder="0" allowfullscreen=""></iframe><iframe style="display: inline-block;" width="280" height="280" src="https://www.youtube.com/embed/rJ0KQjku7T4" frameborder="0" allowfullscreen=""></iframe>
</div>

<p><br /></p>

<p>The figure below plots how the probability that the models place on each note changes during the composition. Note that 0 corresponds to the <em>note off</em> event, and 1 corresponds to <em>no event</em>; these are used for rests and holding notes.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/assets/rl_tuner/note_rnn.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/rl_tuner/q.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/rl_tuner/psi.png" alt="" /></th>
      <th style="text-align: center"><img src="/assets/rl_tuner/g.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Note RNN</td>
      <td style="text-align: center">Q</td>
      <td style="text-align: center"><script type="math/tex">\Psi</script></td>
      <td style="text-align: center">G</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="summary">Summary</h2>
<p>In conclusion, we’ve demonstrated a technique for using RL to refine or <em>tune</em> (*pun intended*) an RNN trained on data. Although some researchers may prefer to train models end-to-end on data, this approach is limited by the quality of the data that can be collected. When the data contains hidden biases, this approach can really be problematic.</p>

<p>We hope you’ve enjoyed this post. If you’re interested in trying out the RL Tuner model for yourself, please check out the <a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/rl-tuner">README</a> file on the Magenta github.</p>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work was done in collaboration with <a href="http://sg717.user.srcf.net/">Shane Gu</a>, <a href="http://learning.eng.cam.ac.uk/Public/Turner/WebHome">Richard E. Turner</a>, and <a href="http://research.google.com/pubs/author39086.html">Douglas Eck</a>. Many thanks also go to my wonderful collaborators on the <a href="https://magenta.tensorflow.org/">Magenta</a> team in <a href="http://research.google.com/teams/brain/">Google Brain</a>, and in particular Curtis (Fjord) Hawthorne and <a href="http://kastnerkyle.github.io/">Kyle Kastner</a> (for his knowledgeable insights and handy spectrogram-movie-producing code).</p>

<!-- Our stuff (may need to change) -->

<!-- Research papers -->

<!-- External links -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  "HTML-CSS": { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
});
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


        Read <a class="post-full-link" href="/2016/10/24/natasha/">full post</a>.
      </li>
    
      
      <li>
        <h2>
          <a class="post-link" href="/2016/10/11/coconet/">Coconet Counterpoint by Convolution</a>
        </h2>
        <span class="post-meta">Oct 11, 2016</span>
        •
         
           Anna Huang
         
         
           (<a href="https://github.com/annahuang"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">annahuang</span></a>
)
         
        
        <p>[TODOs:
* Rewrite abstract, background section
* Prose to explain plot on generating from scratch
* Closing: conclusion, future work …
* Plot to explain blankout process
* Sound clip to show rewriting multiple times improve the generated music
]</p>


        Read <a class="post-full-link" href="/2016/10/11/coconet/">full post</a>.
      </li>
    
  </ul>

</div>
      </div>
    </div>

  </body>

</html>