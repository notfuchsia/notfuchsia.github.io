<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Welcome to Magenta!</title>
  <meta name="description" content="We’re happy to announce Magenta, a project from the Google Brainteam that asks: Can we usemachine learning to create compelling art and music? If so, how? If...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://magenta.tensorflow.org/welcome-to-magenta">
  <link rel="alternate" type="application/rss+xml" title="Magenta" href="https://magenta.tensorflow.org/feed.xml">
  <script src="//www.google.com/js/gweb/analytics/autotrack.js"></script>
  <script>
  new gweb.analytics.AutoTrack({
    profile: 'UA-80107903-1'
  });
  </script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    <a class="site-title" href="/"><span class="site-title"><img src="/assets/magenta-logo.png" height=75></span>
</a -->
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="https://www.github.com/tensorflow/magenta">GitHub</a>
        <a class="page-link" href="/feed.xml">RSS feed</a>
        <a class="page-link" href="https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss">Discuss</a>
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        


      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Welcome to Magenta!</h1>

    <p class="post-meta"><time datetime="2016-06-01T15:00:00+00:00" itemprop="datePublished">Jun 1, 2016</time>
     •
      
        <a href=http://research.google.com/pubs/author39086.html>Douglas Eck</a>
      
      
        (<a href="https://github.com/douglaseck"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">douglaseck</span></a>
)
      
    </span>
    </span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>We’re happy to announce Magenta, a project from the <a href="https://research.google.com/teams/brain/">Google Brain
team</a> that asks: Can we use
machine learning to create compelling art and music? If so, how? If
not, why not?  We’ll use <a href="https://www.tensorflow.org">TensorFlow</a>, and
we’ll release our models and tools in open source on our GitHub. We’ll
also post demos, tutorial blog postings and technical papers. Soon
we’ll begin accepting code contributions from the community at
large. If you’d like to keep up on Magenta as it grows, you can follow
us on our <a href="https://github.com/tensorflow/magenta">GitHub</a> and join our
<a href="https://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss">discussion
group</a>.</p>

<h1 id="what-is-magenta">What is Magenta?</h1>

<p>Magenta has two goals. First, it’s a research project to advance the
state of the art in machine intelligence for music and art
generation. Machine learning has already been used extensively to
understand content, as in speech recognition or translation. With
Magenta, we want to explore the other side—developing algorithms that
can learn how to generate art and music, potentially creating
compelling and artistic content on their own.</p>

<p>Second, Magenta is an attempt to build a community of artists, coders
and machine learning researchers. The core Magenta team will build
open-source infrastructure around TensorFlow for making art and music.
We’ll start with audio and video support, tools for working with
formats like MIDI, and platforms that help artists connect to machine
learning models.  For example, we want to make it super simple to play
music along with a <a href="https://www.youtube.com/watch?v=01V9S8_7A0I&amp;feature=youtu.be">Magenta</a> performance model.</p>

<p>We don’t know what artists and musicians will do with these new tools,
but we’re excited to find out. Look at the history of creative
tools. Daguerre and later Eastman didn’t imagine what <a href="https://en.wikipedia.org/wiki/Annie_Leibovitz">Annie
Liebovitz</a> or <a href="https://en.wikipedia.org/wiki/Richard_Avedon">Richard
Avedon</a> would accomplish
in photography. Surely Rickenbacker and Gibson didn’t have <a href="https://en.wikipedia.org/wiki/Jimi_Hendrix">Jimi
Hendrix</a> or
<a href="https://en.wikipedia.org/wiki/St._Vincent_(musician)">St. Vincent</a> in
mind.  We believe that the models that have worked so well in speech
recognition, translation and image annotation will seed an exciting
new crop of tools for art and music creation.</p>

<p>To start, Magenta is being developed by a small team of researchers
from the Google Brain team.  If you’re a researcher or a coder, you
can check out our alpha-version
<a href="https://www.github.com/tensorflow/magenta">code</a>. Once we have a
stable set of tools and models, we’ll invite external contributors to
check in code to our GitHub. If you’re a musician or an artist (or
aspire to be one—it’s easier than you might think!), we hope you’ll
try using these tools to make some noise or images or videos… or
whatever you like.</p>

<p>Our goal is to build a community where the right people are there to
help out.  If the Magenta tools don’t work for you, let us know.  We
encourage you to join our discussion list and shape how Magenta
evolves.  We’d love to know what you think of our work—as an artist,
musician, researcher, coder, or just an aficionado. You can follow our
progress and check out some of the music and art Magenta helps create
right here on this blog.  As we begin accepting code from community
contributors, the blog will also be open to posts from these
contributors, not just Google Brain team members.</p>

<h1 id="research-themes">Research Themes</h1>

<p>We’ll talk about our research goals in more depth later, via a series
of tutorial blog postings. But here’s a short outline to give an idea
of where we’re heading.</p>

<h2 id="generation">Generation</h2>

<p>Our main goal is to design algorithms that learn how to generate art
and music.  There’s been a lot of great work in image generation from
neural networks, such as
<a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">DeepDream</a>
from A. Mordvintsev et al. at Google and <a href="http://arxiv.org/abs/1508.06576">Neural Style
Transfer</a> from L. Gatys et al. at
U. Tübingen. We believe this area is in its infancy, and expect to see
fast progress here. For those following machine learning closely, it
should be clear that this progress is already well underway.  But
there remain a number of interesting questions: How can we make models
like these truly
<a href="https://en.wikipedia.org/wiki/Generative_model">generative</a>? How can
we better take advantage of user feedback?</p>

<h2 id="attention-and-surprise">Attention and Surprise</h2>

<p>It’s not enough just to sample images or sequences from some learned
distribution.  Art is dynamic! Artists and musicians draw our
attention to one thing at the expense of another. They change their
story over time—is any Beatles album exactly like another?—and there’s
always some element of surprise at play. How do we capture effects
like attention and surprise in a machine learning model? While we
don’t have a complete answer for this question, we can point to some
interesting models such as the <a href="http://arxiv.org/abs/1502.03044">Show, Attend and Tell
model</a> by Xu et al. from the <a href="https://mila.umontreal.ca/en/">MILA
lab</a> in Montreal that learns to control
an attentional lens, using it to generate descriptive sentences of
images.</p>

<h2 id="storytelling">Storytelling</h2>

<p>This leads to perhaps our biggest challenge: combining generation,
attention and surprise to tell a compelling story.  So much
machine-generated music and art is good in small chunks, but lacks any
sort of long-term narrative arc. (To be fair, my own 2002 <a href="http://www.iro.umontreal.ca/~eckdoug/blues/index.html">music
generation
work</a> falls
into this category).  Alternately, some machine generated content does
have long-term structure, but that structure is provided TO rather
than learned BY the algorithm. This is the case, for example, in David
Cope’s very interesting <a href="http://artsites.ucsc.edu/faculty/cope/experiments.htm">Experiments in Musical Intelligence
(EMI)</a>, in
which an AI model deconstructs compositions by human composers, finds
common signatures in them, and recombines them into new works.  The
design of models that learn to construct long narrative arcs is
important not only for music and art generation, but also areas like
language modeling, where it remains a challenge to carry meaning even
across a long paragraph, much less whole stories. Attention models
like the Show, Attend and Tell point to one promising direction, but
this remains a very challenging task.</p>

<h2 id="evaluation">Evaluation</h2>

<p>Evaluating the output of generative models is deceivingly
difficult. The time will come when Magenta has 20 different music
generation models available in open source.  How do we decide which
ones are good?  One option is to compare model output to training data
by measuring
<a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a>.  For
music and art, this doesn’t work very well. As argued very nicely in
<a href="http://arxiv.org/abs/1511.01844">A note on generative models</a> (Theis
et al.), it’s easy to generate outputs that are close in terms of
likelihood, but far in terms of appeal (and vice versa). This
motivates work in artificial adversaries such as <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Generative
Adversarial
Nets</a>
by Goodfellow et al. from MILA in Montreal. In the end, to answer the
evaluation question we need to get Magenta tools in the hands of
artists and musicians, and Magenta media in front of viewers and
listeners. As Magenta evolves, we’ll be working on good ways to
achieve this.</p>

<h2 id="other-google-efforts">Other Google efforts</h2>

<p>Finally, we want to mention other Google efforts and resources related
to Magenta.  The <a href="https://ami.withgoogle.com/">Artists and Machine Intelligence
(AMI)</a> project is connecting with artists
to ask: What do art and technology have to do with each other? What is
machine intelligence, and what does ‘machine intelligence art’ look,
sound and feel like? Check out their
<a href="https://medium.com/artists-and-machine-intelligence">blog</a> for more
about AMI.</p>

<p>The <a href="https://www.google.com/culturalinstitute/home">Google Cultural
Institute</a> is fostering
the discovery of exhibits and collections from museums and archives
all around the world. Via their <a href="https://www.google.com/culturalinstitute/thelab/">Lab at the Cultural Institute</a>, they’re also
connecting directly with artists. As we make TensorFlow/Magenta the
best machine learning platform in the world for art and music
generation, we’ll work closely with both AMI and the Google Cultural
Institute to connect artists with technology. To learn more about our
various efforts, be sure to check out the <a href="http://googleresearch.blogspot.com/">Google Research
Blog</a>.</p>


  </div>

</article>

      </div>
    </div>

  </body>

</html>
