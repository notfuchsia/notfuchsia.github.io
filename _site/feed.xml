<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Not Fuchsia</title>
    <description>Testing blog for editing.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 18 Nov 2016 22:19:33 +0000</pubDate>
    <lastBuildDate>Fri, 18 Nov 2016 22:19:33 +0000</lastBuildDate>
    <generator>Jekyll v3.3.0</generator>
    
      <item>
        <title>Tuning Recurrent Neural Networks with Reinforcement Learning</title>
        <description>&lt;p&gt;We are excited to announce our new &lt;a href=&quot;https://arxiv.org/abs/comingsoon&quot;&gt;RL Tuner algorithm&lt;/a&gt;, a method for enchancing the performance of an LSTM trained on data using Reinforcement Learning (RL). We create an RL reward function that teaches the model to follow certain rules, while still allowing it to retain information learned from data. We use &lt;em&gt;RL Tuner&lt;/em&gt; to teach concepts of music theory to an LSTM trained to generate melodies. The two videos below show samples from the original LSTM model, and the same model enchanced using &lt;em&gt;RL Tuner&lt;/em&gt;.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
	&lt;iframe style=&quot;display: inline-block;margin-right:5px&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/cDcsOokicLw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;iframe style=&quot;display: inline-block;&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/abBfZB5DlSY&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;img style=&quot;margin-right:10px&quot; align=&quot;left&quot; src=&quot;/assets/rl_tuner/note_rnn_model.png&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When I joined &lt;a href=&quot;https://magenta.tensorflow.org/&quot;&gt;Magenta&lt;/a&gt; as an intern this summer, the team was hard at work on developing better ways to train Recurrent Neural Networks (RNNs) to generate sequences of notes. As you may remember from &lt;a href=&quot;https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/&quot;&gt;previous posts&lt;/a&gt;, these models typically consist of a Long Short-Term Memory (LSTM) network trained on monophonic melodies. This means that melodies are fed into the network one note at a time, and it is trained to predict the next note in the sequence. The figure on the left shows a simplified version of this type of network unrolled over time, in which it is being trained on the first 6 notes of “Twinkle Twinkle Little Star”. From now on, I’ll refer to this type of model as a &lt;strong&gt;Note RNN&lt;/strong&gt;.&lt;/p&gt;

&lt;!--With a little hard work and creativity, a *Note RNN* can produce [nice melodies][elliot song].--&gt;
&lt;p&gt;A &lt;em&gt;Note RNN&lt;/em&gt; is conceptually similar to a &lt;em&gt;Character RNN&lt;/em&gt;, a popular model for generating text, one character at a time. While both types of models can produce impressive results, they have some frustrating limitations. They both suffer from common failure modes, such as continually repeating the same token. Further, the sequences produced by the models tend to lack a consistent global structure. To see this more clearly, take a look at the text below, which was generated by a Character RNN trained on Wikipedia markdown data (taken from &lt;a href=&quot;https://arxiv.org/pdf/1308.0850.pdf&quot;&gt;Graves, 2013&lt;/a&gt;):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The ‘'’Rebellion’’’ (‘‘Hyerodent’’) is [[literal]], related mildly older than old half sister, the music, and morrow been much more propellent. All those of [[Hamas (mass)|sausage trafficking]]s were also known as [[Trip class submarine|“Sante” at Serassim]]; “Verra” as 1865-682-831 is related to ballistic missiles. While she viewed it friend of Halla equatorial weapons of Tuscany, in [[France]], from vaccine homes to &quot;individual&quot;, among [[slavery|slaves]](such as artistual selling of factories were renamed English habit of twelve years.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While the model has learned how to correctly spell English words, some markdown syntax, and even mostly correct grammatical structure, sentences don’t seem to follow a consistent thought. The text wonders rapidly from topic to topic, discussing everything from sisters to submarines to Tuscany, slavery, and factories, all in one short paragraph. This type of global incoherence is typical of the melodies produced by a vanilla &lt;em&gt;Note RNN&lt;/em&gt; as well. They do not maintain a consistent musical structure, and can sound wandering and random.&lt;/p&gt;

&lt;p&gt;Music is an interesting test-bed for sequence generation, in that musical compositions adhere to a relatively well-defined set of structural rules. Any beginning music student learns that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases. So what if we could teach a &lt;em&gt;Note RNN&lt;/em&gt; these kinds of musical rules, while still allowing it to learn patterns from music it hears in the world?&lt;/p&gt;

&lt;p&gt;This is the idea behind the &lt;strong&gt;RL Tuner&lt;/strong&gt; model I will describe in this post. We take a trained &lt;em&gt;Note RNN&lt;/em&gt; and teach it concepts of music theory using Reinforcement Learning (RL). RL can allow a network to learn some non-differentiable &lt;em&gt;reward&lt;/em&gt; function. In this case, we define a set of music theory rules, and produce rewards based on whether the model’s compositions adhere to those rules. However, to ensure that the model can remember the note probabilities it originally learned from data, we keep a second, fixed copy of the &lt;em&gt;Note RNN&lt;/em&gt; which we call the &lt;em&gt;Reward RNN&lt;/em&gt;. The &lt;em&gt;Reward RNN&lt;/em&gt; is used to compute the probability of playing the next note as learned by the original &lt;em&gt;Note RNN&lt;/em&gt;. We augment our music theory rewards with this probability value, so that the total reward reflects both our music theory constraints and information learned from data.&lt;/p&gt;

&lt;p&gt;We show that this approach allows the model to maintain information about the note probabilities learned from data, while significantly improving the behaviors of the &lt;em&gt;Note RNN&lt;/em&gt; targeted by the music theory rewards. For example, before training with RL, 63.3% of notes produced by the &lt;em&gt;Note RNN&lt;/em&gt; belonged to some excessively repeated segment of notes; after RL, 0.0-0.03% of notes were excessively repeated. Since excessively repeating tokens is a problem in other domains as well (e.g. text generation), we believe our approach could have broader applications. But does it actually work to produce better music? We conducted a user study and found that people find the compositions produced by our three RL models significantly more musically pleasing than those of the original &lt;em&gt;Note RNN&lt;/em&gt;. But we encourage you to judge for yourself; samples from each of the models will be provided later in this post.&lt;/p&gt;

&lt;p&gt;The models, derivations, and results are all described in our recent &lt;a href=&quot;https://arxiv.org/abs/comingsoon&quot;&gt;research paper&lt;/a&gt;, written by myself, &lt;a href=&quot;http://sg717.user.srcf.net/&quot;&gt;Shane Gu&lt;/a&gt;, &lt;a href=&quot;http://learning.eng.cam.ac.uk/Public/Turner/WebHome&quot;&gt;Richard E. Turner&lt;/a&gt;, and Douglas Eck&lt;sup&gt;&lt;a href=&quot;#footnote-nipsrl&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. &lt;a href=&quot;https://github.com/tensorflow/magenta/tree/master/magenta/models/rl-tuner&quot;&gt;Code&lt;/a&gt; to run this model is also available on the Magenta github repo; please try it out! The music theory rules implemented for the model are only a first attempt, and could easily be improved by someone with musical training.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnote-nipsrl&quot;&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/sub&gt;&lt;/a&gt; &lt;sub&gt;&lt;sup&gt;A version of this work was accepted at the &lt;a href=&quot;https://sites.google.com/site/deeprlnips2016/&quot;&gt;NIPS 2016 Deep Reinforcement Learning Workshop&lt;/a&gt;.&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;h2 id=&quot;background-reinforcement-learning-and-deep-q-learning&quot;&gt;Background: Reinforcement Learning and Deep Q-Learning&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This section will give a brief introduction to some ideas behind RL and Deep Q Networks (DQNs). If you’re familiar with these topics you may wish to skip ahead.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In reinforcement learning (RL), an agent interacts with an environment. Given the state of the enviornment &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, the agent takes an action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;, receives a reward &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;, and the environment transitions to a new state, &lt;script type=&quot;math/tex&quot;&gt;s'&lt;/script&gt;. The goal of the agent is to maximize reward, which is usually some clear signal from the environment, such as points in a game.&lt;/p&gt;

&lt;p&gt;The rules for how the agent chooses to act in the environment define a &lt;em&gt;policy&lt;/em&gt;. To learn the most effective policy, the agent can’t just greedily maximize the reward it will receive after the next action, but must instead consider the total cumulative reward it can expect to receive over a course of actions occurring in the future. Because future rewards are typically uncertain if the environment has random effects, a discount factor of &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; is applied to the reward for each timestep in the future. If &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt; is the reward received at timestep &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt; is the total future discounted &lt;em&gt;return&lt;/em&gt;:
\begin{align}
R_t = \sum^T_{t’=t}\gamma^{t’-t}r_{t’} 
\end{align}&lt;/p&gt;

&lt;p&gt;In Q-learning, the goal is to learn a Q function that gives the maximum expected discounted future return for taking any action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; in state &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, and continuing to act optimally at each step in the future. Therefore the optimal Q function, &lt;script type=&quot;math/tex&quot;&gt;Q^*&lt;/script&gt; is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
Q^*(s, a) = max_\pi \mathbb{E}[R_t|s_t = s, a_t = a, \pi]
\end{align}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is the policy mapping each state to a probability distributions over the next action. To learn &lt;script type=&quot;math/tex&quot;&gt;Q*&lt;/script&gt;, we can apply an iterative update based on the Bellman equation:
\begin{align}
Q_{i+1}(s, a) = \mathbb{E}[r + \gamma max_{a’}Q_i(s’,a’)|s,a]
\end{align}
where &lt;em&gt;r&lt;/em&gt; is the reward received for taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt;.  This value iteration method will converge to &lt;script type=&quot;math/tex&quot;&gt;Q^*&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;i \rightarrow \infty&lt;/script&gt;. While learning, it is important to explore the space of possible actions, either by occasionally choosing random actions, or sampling an action based on the values defined by the &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; function. Once the &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; function has been learned, the optimal policy can be obtained by simply choosing the action with the highest &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; value at every step.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot;&gt;Deep Q Learning&lt;/a&gt;, a neural network called the Deep Q-network (DQN) is used to approximate the &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; function, &lt;script type=&quot;math/tex&quot;&gt;Q(s, a; \theta) \approx Q^*(s, a)&lt;/script&gt;. The network parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; are learned by applying stochastic gradient descent (SGD) updates with respect to the following loss function:
\begin{align} 
\label{eq:qloss}
L_t(\theta_t) = (r_t + \gamma \max_{a’}Q(s’,a’;\theta_{t-1}) - Q(s,a;\theta_t))^2
\end{align}
The first two terms are the &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; function the network is trying to learn: the actual reward received at step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt;, plus the discounted future return estimated by the &lt;em&gt;Q-network&lt;/em&gt; parameters at step &lt;script type=&quot;math/tex&quot;&gt;t-1&lt;/script&gt;. The loss function computes the difference between this desired &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; value, and the actual value output by the &lt;em&gt;Q-network&lt;/em&gt; at step &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. Essentially, this is the prediction error in estimating the &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; function made by the &lt;em&gt;Q-network&lt;/em&gt;. Importantly, the parameters for the previous generation of the network (&lt;script type=&quot;math/tex&quot;&gt;\theta_{t-1}&lt;/script&gt;) are held fixed, and not updated by SGD.&lt;/p&gt;

&lt;!--While interacting with the environment, the $$Q$$-learning agent typically follows an $$\epsilon$$-greedy policy. This means that it will greedily choose the action with the highest Q value with probability $$1-\epsilon$$, and choose a random action with probability $$\epsilon$$. This off-policy learning method ensures a reasonably trade-off between exploration of the action space, and exploitation of the most efficient strategies learned so far. --&gt;

&lt;p&gt;Several techniques are required for a DQN to work effectively. As the agent interacts with the environment, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;s_t,a_t,r_t,s_{t+1}&gt; %]]&gt;&lt;/script&gt; tuples it experiences are stored in an &lt;em&gt;experience buffer&lt;/em&gt;. Training the &lt;em&gt;Q-network&lt;/em&gt; is accomplished by randomly sampling batches from the &lt;em&gt;experience buffer&lt;/em&gt; to compute the loss. The &lt;em&gt;experience buffer&lt;/em&gt; is essential for learning; if the agent was trained using consecutive samples of experience, the samples would be highly correlated, updates would have high variance, and the network parameters could become stuck in a local minimum or diverge.&lt;/p&gt;

&lt;p&gt;Further optimizations to the DQN algorithm have been proposed that help enhance learning and ensure stability. One of these is &lt;a href=&quot;http://www.aaai.org/Conferences/AAAI/2016/Papers/12vanHasselt12389.pdf&quot;&gt;Deep Double Q-learning&lt;/a&gt;, in which a second, &lt;em&gt;Target Q-network&lt;/em&gt; is used to estimate expected future return, while the &lt;em&gt;Q-network&lt;/em&gt; is used to choose the next action. Since Q-learning has been shown to learn unrealistically high action values because it estimates maximum expected return, having a second Q-network can lead to more realistic estimates and better performance.&lt;/p&gt;

&lt;h2 id=&quot;rl-tuner&quot;&gt;RL Tuner&lt;/h2&gt;
&lt;p&gt;As described above, the main idea behind the RL Tuner model is to take an RNN trained on data, and refine it using RL. The model uses a standard DQN implementation, complete with an &lt;em&gt;experience buffer&lt;/em&gt; and &lt;em&gt;Target Q-network&lt;/em&gt;. A trained &lt;em&gt;Note RNN&lt;/em&gt; is used to supply the initial values of the weights in the &lt;em&gt;Q-network&lt;/em&gt; and &lt;em&gt;Target Q-network&lt;/em&gt;, and a third copy is  used as the &lt;em&gt;Reward RNN&lt;/em&gt;. The &lt;em&gt;Reward RNN&lt;/em&gt; is held fixed during training, and is used to supply part of the reward function used to train the model. The figure below illustrates these ideas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rl_tuner/rl_rnn_diagram.png&quot; alt=&quot;Model diagram&quot; title=&quot;Model diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To formulate musical composition as an RL problem, we treat choosing the next note as taking an action &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;. The state of the environment &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; consists of the state of the composition so far, as well as the internal LSTM state of the &lt;em&gt;Q-network&lt;/em&gt; and &lt;em&gt;Reward RNN&lt;/em&gt;. The reward function is a combination of both music theory rules and probabilities learned from data. The music theory reward &lt;script type=&quot;math/tex&quot;&gt;r_{MT}(a,s)&lt;/script&gt; is calculated by a set of functions (described in the next section) that constrain the model to adhere to certain rules, such as playing in the same key. However, it is necessary that the model still be “creative” rather than learning a simple composition that can easily exploit these rewards. Therefore, the &lt;em&gt;Reward RNN&lt;/em&gt; is used to compute &lt;script type=&quot;math/tex&quot;&gt;p(a&lt;/script&gt;|&lt;script type=&quot;math/tex&quot;&gt;s)&lt;/script&gt;, the probability of playing the next note &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; given the composition &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; as originally learned from actual songs. The total reward given at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align} 
\label{eq:reward}
r_t(a,s) = \log p(a|s) + \frac{1}{c}r_{MT}(a,s)
\end{align}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is a constant controlling the emphasis placed on the music theory reward. So now, we see that the new loss function for our model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\label{eq:melody_dqn_loss}
L_t(\theta_t) = (\log p(a|s) + \frac{1}{c}r_{MT}(a,s) + \gamma \max_{a'}Q(s',a';\theta_{t-1}) - Q(s,a;\theta_t))^2
\end{align}&lt;/script&gt;

&lt;p&gt;This modified loss function forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have the high probability in the original data.&lt;/p&gt;

&lt;h3 id=&quot;for-the-mathematically-inclined&quot;&gt;For the mathematically inclined…&lt;/h3&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/abs/comingsoon&quot;&gt;our paper&lt;/a&gt; we show that the loss function described above can be related to an approximation of the Stochastic Optimal Control objective, leading to a RL cost with an additional penalty applied to KL-divergence from a prior policy. If we think of the probabilities learned by the &lt;em&gt;Note RNN&lt;/em&gt; as the prior policy &lt;script type=&quot;math/tex&quot;&gt;p(\cdot&lt;/script&gt;|&lt;script type=&quot;math/tex&quot;&gt;s)&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}&lt;/script&gt; as the policy learned by the model, then this would be equivalent to learning the following function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align} 
\mathbb{E}_{\pi}[\sum_t r(s_t,a_t)/c - KL[\pi_\theta(\cdot|s_t)||p(\cdot|s_t)]]
\end{align}&lt;/script&gt;

&lt;p&gt;This is not exactly the same as our method, because we are missing the entropy term in the KL-divergence function. This led us to implement two other KL-regularized variants of Q-learning: &lt;a href=&quot;http://homepages.inf.ed.ac.uk/svijayak/publications/rawlik-RSS2012.pdf&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; learning&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/1512.08562.pdf&quot;&gt;G learning&lt;/a&gt;. The loss function for &lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; learning is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
&amp;L_{\Psi}(\theta)= \mathbb{E}[(\log p(a|s) + \frac{1}{c}r_{MT}(s,a) + \gamma \log \sum_{a'} e^{\Psi(s',a';\theta^-)} - \Psi(s,a;\theta))^2]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;And the loss function for G learning is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
&amp;L_G(\theta)= \mathbb{E}_\beta[(\frac{1}{c}r_{MT}(s,a)+ \gamma \log \sum_{a'} e^{\log p(a'|s')+G(s',a';\theta^-)} - G(s,a;\theta))^2]
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;music-theory-rewards&quot;&gt;Music Theory Rewards&lt;/h2&gt;
&lt;p&gt;The rules we implemented to make sure our model conformed to music theory were based on some domain knowledge, and the book &lt;a href=&quot;http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents&quot;&gt;“A Practical Approach to Eighteenth-Century Counterpoint” by Robert Gauldin&lt;/a&gt;. We are by no means claiming that these rules are necessary for good compositions, exhaustive, or even particularly creative. They simply allow us to constrain our model to adhere to some sort of consistent structure. We encourage any interested readers to experiment with different rules and see what types of results they can produce. For now, the rules that we chose encourage the compositions produced by our model to have the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Stay in key&lt;/strong&gt;: Notes should belong to the same key. For example, if the desired key is C-major, a B-flat would not be an acceptable note.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Begin and end with the tonic note&lt;/strong&gt;: The first note of the composition, and the first note of the final bar should be the tonic note of the key; e.g. if the key is C-major, this note would be middle C.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Avoid excessively repeated notes&lt;/strong&gt;: Unless a rest is introduced or a note is held, a single tone should not be repeated more than four times in a row. While the number four can be considered a rough heuristic, avoiding excessively repeated notes and static melodic contours is &lt;a href=&quot;http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents&quot;&gt;Gauldin’s&lt;/a&gt; first rule of melodic composition.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prefer harmonious intervals&lt;/strong&gt;: The composition should avoid awkward intervals like augmented sevenths, or large jumps of more than an octave. Gauldin also indicates good compositions should move by a mixture of small steps and larger harmonic intervals, with emphasis on the former; the reward values for intervals reflect these requirements.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Resolve large leaps&lt;/strong&gt;: When the composition leaps several pitches in one direction, it should eventually be resolved by a leap back or gradual movement in the opposite direction. Leaping twice in the same direction is negatively rewarded.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Avoid continuously repeating extrema notes&lt;/strong&gt;: The highest note of the composition should be unique, as should the lowest note.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Avoid high auto-correlation&lt;/strong&gt;: To encourage variety, the reward function penalizes a melody if it is highly correlated with itself at a lag of 1, 2, or 3 beats.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Play motifs&lt;/strong&gt;: A musical motif is a succession of notes representing the shortest musical “idea”; in our implementation, it is defined as a bar of music with three or more unique notes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Play repeated motifs&lt;/strong&gt;: Because &lt;a href=&quot;https://secureweb.mcgill.ca/spl/files/spl/livingstoneemotion2012.pdf&quot;&gt;repetition has been shown to be key to emotional engagement with music&lt;/a&gt;, we tried to train the model to repeat motifs that it had previously introduced.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;To see if our models actually learned what we were trying to teach them, we computed statistics about how many of the notes and compositions generated by the models adhered to our music theory rules. The results are shown in the table below, and represent statistics about 100,000 compositions randomly generated by each model. The top section shows behaviors we want to decrease, and the bottom show behaviors we want to increase. Bolded entries are significantly better than the basic &lt;em&gt;Note RNN&lt;/em&gt;.&lt;/p&gt;

&lt;table style=&quot;width:100%&quot;&gt;
	&lt;thead style=&quot;border-bottom:1px&quot;&gt;
		&lt;tr&gt;
			&lt;th style=&quot;text-align:left&quot;&gt; Behavior   &lt;/th&gt;
			&lt;th style=&quot;text-align:left&quot;&gt; Note RNN &lt;/th&gt;
			&lt;th style=&quot;text-align:left&quot;&gt; Q   &lt;/th&gt;
			&lt;th style=&quot;text-align:left&quot;&gt; Psi &lt;/th&gt;
			&lt;th style=&quot;text-align:left&quot;&gt; G &lt;/th&gt;
		&lt;/tr&gt;
	&lt;/thead&gt;
	&lt;tr&gt;
		&lt;td colspan=&quot;5&quot; style=&quot;border-bottom:1px solid black;&quot;&gt;&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Notes excessively repeated   &lt;/td&gt;
		&lt;td&gt; 63.3% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.0%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.02%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.03%&lt;/strong&gt; &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Notes not in key   &lt;/td&gt;
		&lt;td&gt; 0.1% &lt;/td&gt;
		&lt;td&gt; 1.0% &lt;/td&gt;
		&lt;td&gt; 0.6% &lt;/td&gt;
		&lt;td&gt; 28.7% &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Mean autocorrelation (lag 1,2,3)   &lt;/td&gt;
		&lt;td&gt; -.16, .14, -.13 &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;-.11, .03, .03&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;-.10, -.01, .01&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; .55, .31, .17 &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td colspan=&quot;5&quot; style=&quot;border-bottom:1px solid black;&quot;&gt;&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Leaps resolved   &lt;/td&gt;
		&lt;td&gt; 77.2% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;91.1%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;90.0%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; 52.2% &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Compositions starting with tonic   &lt;/td&gt;
		&lt;td&gt; 0.9% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;28.8%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;28.7%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; 0.0% &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Compositions with unique max note   &lt;/td&gt;
		&lt;td&gt; 64.7% &lt;/td&gt;
		&lt;td&gt; 56.4% &lt;/td&gt;
		&lt;td&gt; 59.4% &lt;/td&gt;
		&lt;td&gt; 37.1% &lt;/td&gt;
	&lt;/tr&gt;
		&lt;tr&gt;
		&lt;td&gt; Compositions with unique min note   &lt;/td&gt;
		&lt;td&gt; 49.4% &lt;/td&gt;
		&lt;td&gt; 51.9% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;58.3%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;56.5%&lt;/strong&gt; &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Notes in motif   &lt;/td&gt;
		&lt;td&gt; 5.9% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;75.7%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;73.8%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;69.3%&lt;/strong&gt; &lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td&gt; Notes in repeated motif   &lt;/td&gt;
		&lt;td&gt; 0.007% &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.11%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.09%&lt;/strong&gt; &lt;/td&gt;
		&lt;td&gt; &lt;strong&gt;0.01%&lt;/strong&gt; &lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
These results show that the targeted behaviors have significantly improved in the RL models as compared to the original &lt;em&gt;Note RNN&lt;/em&gt;. But how do we know that learning these rules didn’t cause the models to forget what they learned from data? The figures below plot the rewards received by each model over time, broken down into the &lt;script type=&quot;math/tex&quot;&gt;\log p(a&lt;/script&gt;|&lt;script type=&quot;math/tex&quot;&gt;s)&lt;/script&gt; rewards from the Note RNN (on the left), and the music theory rewards &lt;script type=&quot;math/tex&quot;&gt;r_{MT}&lt;/script&gt; (on the right). Each model is compared to a baseline &lt;em&gt;RL only&lt;/em&gt; model that was trained using only the music theory rewards, and no information about the data probabilities. We see that compared to the &lt;em&gt;RL only&lt;/em&gt; model, the RL Tuner models maintain much higher &lt;script type=&quot;math/tex&quot;&gt;\log p(a&lt;/script&gt;|&lt;script type=&quot;math/tex&quot;&gt;s)&lt;/script&gt;, while still learning the music theory rewards.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/rewards_note_rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/rewards_music_theory.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Note RNN rewards&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Music theory rewards&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;
Even so, it’s not clear that learning this somewhat arbitrary set of music theory rules will lead to better sounding compositions. To find out whether the models have improved over the original &lt;em&gt;Note RNN&lt;/em&gt;, we asked Mechanical Turk workers to rate which of two randomly selected compositions they preferred. The figure below shows the number of times a composition from each model was selected as the winner. All three RL Tuner models significantly outperformed the &lt;em&gt;Note RNN&lt;/em&gt;, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p&lt;.001 %]]&gt;&lt;/script&gt;. In addition, both the &lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; models were statistically significantly better than the &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; model, although not significantly different from each other.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/user_study.png&quot; width=&quot;400&quot; /&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
But you don’t need to take our word for it! The compositions used in the study are all available &lt;a href=&quot;goo.gl/XIYt9m&quot;&gt;here&lt;/a&gt; - please check them out! Below, you can listen to a composition from each model. In the top row are compositions from the &lt;em&gt;Note RNN&lt;/em&gt; and &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;; in the bottom row are &lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;. You can see that the &lt;em&gt;Note RNN&lt;/em&gt; plays the same note repeatedly, while the &lt;em&gt;RL Tuner&lt;/em&gt; models sound much more varied and interesting. The &lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; are best at playing within the music theory contraints, staying firmly in key and frequently choosing more harmonious interval steps. Still, we can tell that the models have retained information about the training songs. The &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; sample ends with a riff that sounds very familiar!&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
	&lt;iframe style=&quot;display: inline-block;margin-right:5px&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/c5wm9NR1QiU&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;iframe style=&quot;display: inline-block;&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/HjCxS53Ta14&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
	&lt;iframe style=&quot;display: inline-block;margin-right:5px&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/PavKsrzFAEI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;iframe style=&quot;display: inline-block;&quot; width=&quot;280&quot; height=&quot;280&quot; src=&quot;https://www.youtube.com/embed/rJ0KQjku7T4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The figure below plots how the probability that the models place on each note changes during the composition. Note that 0 corresponds to the &lt;em&gt;note off&lt;/em&gt; event, and 1 corresponds to &lt;em&gt;no event&lt;/em&gt;; these are used for rests and holding notes.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/note_rnn.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/q.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/psi.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/rl_tuner/g.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Note RNN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Q&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\Psi&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;G&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In conclusion, we’ve demonstrated a technique for using RL to refine or &lt;em&gt;tune&lt;/em&gt; (*pun intended*) an RNN trained on data. Although some researchers may prefer to train models end-to-end on data, this approach is limited by the quality of the data that can be collected. When the data contains hidden biases, this approach can really be problematic.&lt;/p&gt;

&lt;p&gt;We hope you’ve enjoyed this post. If you’re interested in trying out the RL Tuner model for yourself, please check out the &lt;a href=&quot;https://github.com/tensorflow/magenta/tree/master/magenta/models/rl-tuner&quot;&gt;README&lt;/a&gt; file on the Magenta github.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This work was done in collaboration with &lt;a href=&quot;http://sg717.user.srcf.net/&quot;&gt;Shane Gu&lt;/a&gt;, &lt;a href=&quot;http://learning.eng.cam.ac.uk/Public/Turner/WebHome&quot;&gt;Richard E. Turner&lt;/a&gt;, and &lt;a href=&quot;http://research.google.com/pubs/author39086.html&quot;&gt;Douglas Eck&lt;/a&gt;. Many thanks also go to my wonderful collaborators on the &lt;a href=&quot;https://magenta.tensorflow.org/&quot;&gt;Magenta&lt;/a&gt; team in &lt;a href=&quot;http://research.google.com/teams/brain/&quot;&gt;Google Brain&lt;/a&gt;, and in particular Curtis (Fjord) Hawthorne and &lt;a href=&quot;http://kastnerkyle.github.io/&quot;&gt;Kyle Kastner&lt;/a&gt; (for his knowledgeable insights and handy spectrogram-movie-producing code).&lt;/p&gt;

&lt;!-- Our stuff (may need to change) --&gt;

&lt;!-- Research papers --&gt;

&lt;!-- External links --&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  CommonHTML: { linebreaks: { automatic: true } },
  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
         SVG: { linebreaks: { automatic: true } }
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Mon, 24 Oct 2016 15:00:00 +0000</pubDate>
        <link>http://localhost:4000/2016/10/24/natasha/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/10/24/natasha/</guid>
        
        <category>magenta,melody,rnn,lstm,reinforcement,music</category>
        
        
      </item>
    
      <item>
        <title>Coconet Counterpoint by Convolution</title>
        <description>&lt;p&gt;[TODOs:
* Rewrite abstract, background section
* Prose to explain plot on generating from scratch
* Closing: conclusion, future work …
* Plot to explain blankout process
* Sound clip to show rewriting multiple times improve the generated music
]&lt;/p&gt;

&lt;!--Recently, WaveNet showed that we can generate music on the waveform level
sample by sample, and also time travel from romantic nocturnes to atonal
bombastic mysticism in seconds.  Surprisingly, this was achieved not by RNNs,
but CNNs, demonstrating that they can also successfully capture short and long
term structure in sequential data such as speech and music audio.--&gt;

&lt;!--Here, we introduce, CoCoNet, where we use convolutions to model counterpoint
in music on the symbolic level, treating music as multiple simultaneous
sequences.  We aim to generate music with a sense of flow, phrasing and
coherence that spans over a minute by conditioning on the long-term structure of
existing Bach chorales, by rewriting them voice by voice until all parts are
generated.  We frame this music generation problem as an inpainting task, where
we (learn to) generate by conditioning on arbitrary context.  This frees us from
needing to generate in a chronological order (such as in RNNs), which can be too
constraining for symbolic music generation.  --&gt;

&lt;!--Here, we zoom out and then zoom in to music in a different way.  We zoom out
because we're looking at music from the symbolic level where we already have
notes, not just the waveforms.  We're zooming in because we're not treating
music as a 1D sequence but the multiple simultaneous instrumental/vocal lines
that make up a sequence. --&gt;

&lt;!--We introduce a method for music score generation using deep convolutional
inpainting.--&gt;

&lt;p&gt;Previously, music was often modeled sequentially and assumed to be generated
from left to right.  In this work, we frame music generation as an inpainting
task, where models are trained by predicting fill-ins for randomly blanked out
scores.  Blank outs within voices, across voices, and across time allow the
model to learn conditional distributions that capture the interdependencies
necessary to compose counterpoints.  This allows models to learn conditional
distributions beyond chronological ones and thus allowing the model to generate
in any order.  We introduce, Coconet, that composes counterpoint through
convolution.  It can generate new pieces by rewriting existing pieces voice by
voice, or it can generate from scratch, by writing counterpoint against itself,
and then rewriting itself over and over again, akin to Gibbs sampling.  In this
blog, we’ll highlight some generated examples and give a walkthrough of how the
model generates step by step to gain some insight on how the model thinks about
music.&lt;/p&gt;

&lt;h2 id=&quot;lets-start-with-a-turing-test--a-nameforwardsamplea&quot;&gt;Let’s start with a ‘Turing’ test  &lt;a name=&quot;forwardSample&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;We have two synthesized MIDI excerpts here, each 16 measures long.  Which do you
think is J. S. Bach himself and which is generated by our Coconet model?&lt;/p&gt;

&lt;!--Technically it’s not a proper Turing test because we don’t get to interact
and play with the model to see how it responds musically to judge if it's a
machine or not.  But I'll be releasing my code on Magenta's
[github](magenta-github) soon so you'll definitely get to play with it if you're
interested. --&gt;

&lt;iframe width=&quot;300&quot; height=&quot;200&quot; src=&quot;https://www.youtube.com/embed/VYTI7XbqC4Y&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;[TODO: the forward generation example is cut off at the end.]  &amp;lt;iframe
width=&quot;300&quot; height=&quot;200&quot; src=&quot;https://www.youtube.com/embed/JBVGeZuE5gk&quot;
frameborder=&quot;0&quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&lt;/p&gt;

&lt;!--### How did we encode the music and what are the implications on how the
music can be modeled and generated?  Since there are multiple melodic lines
playing at the same time, the music looks more like a grid (think piano roll, a
2D matrix where rows are pitches and columns are discretized time steps) then a
single stream of events.  That means to generate it we need to choose some
ordering for generating the notes.  This turns out to not have a simple answer
because there are many interdependencies within a voice, across voices, and
across time.  In a way, the dependencies are cyclic, and choosing a particular
ordering means we're deciding certain notes do not depend on each other.  We'll
further discuss this challenge below and show how we manage to get around this
in some way.  We experimented with a few different orderings but here we'll
first give a glimpse of the particular procedure we used to generate the excerpt
you heard above.  --&gt;

&lt;h3 id=&quot;how-was-the-excerpt-generated--through-rewriting&quot;&gt;How was the excerpt generated?  Through rewriting.&lt;/h3&gt;
&lt;p&gt;The Coconet chorale you heard above was generated by rewriting an existing Bach
chorale voice by voice.  The generation process starts by randomly picking a
voice to blank out.  Then, the model generates that voice sequentially from left
to right.  At each time step, the generated pitch is fed back into the model so
that the next prediction is aware of all that came before.  When this voice is
completed, another random voice is blanked out, and this time the model
generates by ‘hearing’ two original Bach voices and the one it just generated.
This procedure is repeated until all notes are the result of machine generation.&lt;/p&gt;

&lt;h4 id=&quot;was-it-successful--generating-left-to-right-could-be-too-constraining&quot;&gt;Was it successful?  Generating left to right could be too constraining.&lt;/h4&gt;
&lt;p&gt;Listen for how the melody in the generated chorale starts by taking an
octave-long journey up the scale.  This is rather atypical line for the style
we’re modeling, where lines would take a turn after moving in the same
directions for a few steps in a row, partly to provide balance and partly so
that the voices don’t run out of range.  This could be due to the model being
too constrained by what came before.  As the piece was generated from left to
right, without knowing where to go towards, the easiest way not to make a
mistake in continuing a line that involved a few stepwise motions up already
would be to repeat that motion with notes up the scale.&lt;/p&gt;

&lt;p&gt;Imagine a Jazz musician only playing melodies that were all scale-like runs.
It’s definitely not inferior because the musician will probably be using other
aspects It would still be a successful piece because there’s many other
components that could make it shine such as how the voices complement each
other, how the harmony.  But we want a model that’s able to make interesting
melodies too, or in this case melodies that resemble those in Bach chorales,
which would often make several turns before exhausting an octave.&lt;/p&gt;

&lt;p&gt;Now that we have a sense of what’s at stake, we’ll start the main part of this
blog by first giving some background on some of the challenges in using existing
techniques to model music, touching on representation and model assumptions.
We’ll describe how we came to frame music generation as an inpainting task.
We’ll illustrate how an inpainting model learns conditional distributions that
go beyond left to right.  This will in turn allow us to condition on any
combination of notes that was already in a partial score, and generate the
missing parts in any order.  We’ll walk you through an example of how the model
generates from an empty canvas and it can rewrite itself multiple times to
improve the quality of the generated music.  This walkthrough will help us
understand how the model has learned to think about musical relationships such
as harmony.  We’ll end on an excerpt of a ‘new chorale’, similar to what we just
heard, but with notes generated in non-chronological order.  Let us know what
you think of the differences!&lt;/p&gt;

&lt;p&gt;##Background: What makes it hard to apply existing models to music generation?&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-fit-multiple-instruments-into-one-sequence-of-events--we-dont&quot;&gt;How do we fit multiple instruments into one sequence of events?  We don’t.&lt;/h3&gt;
&lt;!--###The challenge of modeling polyphony--&gt;

&lt;!--### How did we encode the music and what are the implications on how the
music can be modeled and generated?  Since there are multiple melodic lines
playing at the same time, the music looks more like a grid (think piano roll, a
2D matrix where rows are pitches and columns are discretized time steps) then a
single stream of events.  That means to generate it we need to choose some
ordering for generating the notes.  This turns out to not have a simple answer
because there are many interdependencies within a voice, across voices, and
across time.  In a way, the dependencies are cyclic, and choosing a particular
ordering means we're deciding certain notes do not depend on each other.  We'll
further discuss this challenge below and show how we manage to get around this
in some way.  We experimented with a few different orderings but here we'll
first give a glimpse of the particular procedure we used to generate the excerpt
you heard above.  --&gt;

&lt;p&gt;One of the challenges in adapting language models such as RNNs for music
modeling is that speech and language only have one stream of events while music
has multiple instruments and voices playing at the same time, multiple of them
playing at the same time one music often has more than one sequence of events
happening at the same time while language has one.  For example Bach chorales
are polyphonic and the key to success in this style is that the voices nicely
coordinate with each other and that they’re also individually interesting.  To
address this challenge, researchers have come up with different ways to
represent music or to augment existing language models.  For example the author
of BachBot, Liang (2016) serializes four-part Bach chorales in SATB order at
each time step, while Allan and Williams (2005) build a vocabulary of pitch
combinations that are heard simultaneously and encode them as one-hot vectors
[TODO: need to add Mary Farbood’s work from 2001 on HMMs for counterpoint].
Boulanger-Lewandowski et al. (2012) adopts a piano roll representation where
rows are pitches and columns are quantized time.  To predict a multi-hot vector
at each time step and model how well different pitches go together, they augment
the prediction layer of an RNN by replacing the softmax with a Restricted
Boltzmann Machine (RBM).&lt;/p&gt;

&lt;h3 id=&quot;the-problem-of-forward-conditioning-and-generation-another-challenge-in&quot;&gt;The problem of forward conditioning and generation Another challenge in&lt;/h3&gt;
&lt;p&gt;adapting language models for modeling music is that they often assume a
chronological ordering when factorizing the joint distribution over a sequence
$\mathbf{x}$, where $x_t$ is a categorical variable for the pitch it takes on at
time step $t$, and $T$ being the length of the sequence.&lt;/p&gt;

&lt;center&gt;$p(\mathbf{x}) =  \prod^T_{t=1}p(x_t|x_1, ..., x_{t-1})$ &lt;/center&gt;

&lt;p&gt;This means the current timestep is only conditioned on previous timesteps,
theoretically all of them.  This poses a serious constraint on generation
because we can only generate forward and can not condition on where we might
want to end up.  Connecting this problem back to music, musicians often want
more expressive ways of interacting with a generative model, for example, one
might have the beginning and end of a phrase worked out and want a model to
brainstorm what are the possible ways of connecting the two.  With RNNs, we can
not just sample forward in an unconstrained fashion, dynamic programming can be
expansive, greedy search is non-ideal for constructing long sequences, beam
search could be a heuristic to try out.&lt;/p&gt;

&lt;p&gt;[TODO: Mention accumulation of error here or later when we describe how we sample? Probably the later so that it’s less abstract.]&lt;/p&gt;

&lt;h2 id=&quot;modeling-music-generation-as-an-inpainting-task&quot;&gt;Modeling music generation as an inpainting task&lt;/h2&gt;

&lt;p&gt;How can we generate in a way that accounts for everything that’s already been
written in a score?  Better yet, how can we train a model to do this instead of
trying to achieve this post-hoc.  This is our goal.  We want to build a model
that can fill in any missing parts given arbitrary context.  This is the task of
inpainting, which was first introduced by &lt;a href=&quot;context-encoder&quot;&gt;Pathak et al.&lt;/a&gt; this
year in the context of image generation, where random parts of an image are
blanked out and the model learns to reconstruct what could have been there.
This setup can be very powerful because during training each time we randomly
blank out a part, we are asking the model to construct features that would allow
it to predict the missing parts given the existing context.  These could be
local edges that help predict surrounding pixels or semantic features when the
blank outs and context are further away.  And indeed this approach to modeling
has been shown to learn features that yield competitive and better results in a
number of image classification, detection and segmentation tasks [Pathak 2016].
When applied to sequence generation such as music, we can think of the model
learning both forward conditionals and many more in different directions because
how we blank out the input determines what conditional distributions the model
is learning.  For example, if we blanked out timesteps $t$ and $t+1$ in the
middle of a sequences, then we are learning the following condition
distribution.&lt;/p&gt;

&lt;center&gt;$p(x_t, x_{t+1}| x_1, …, x_{t-1}, x_{t+2}, …, x_{T})$&lt;/center&gt;

&lt;p&gt;This allows us to “create” combinatorial amounts of data just by randomly
blanking out different parts.  Subsequently, this allows the model to learn a
combinatorial number of conditional distributions, as if it was learning an
ensemble of different factorizations of the joint distribution.&lt;/p&gt;

&lt;h2 id=&quot;our-model-deep-residual-convolutional-inpainting&quot;&gt;Our model: Deep residual convolutional inpainting&lt;/h2&gt;

&lt;p&gt;Our inpainting model is a deep residual convolutional neural network that takes
partial piano rolls as input and produces a probability distribution over
completed piano rolls.  The basic building blocks are convolutional layers where
we convolve 2D filters across piano rolls hence treating pitch and time as
invariant.  This is partly a reasonable assumption as the identity of a piece is
usually preserved through transposition and similar temporal dynamics hold
regardless of if we’re one or two minutes into a piece.  But this assumption
does miss out on modeling a lot of musical properties, for example each voice in
SATB such as sopranos do have a limited range, and in some keys the melodic line
will reach that boundary more often and have to inflect.  Along the time axis,
locally stressed and unstressed beats would have different harmonic emphasis,
and more globally different sections can have very different characters.  But
sometimes we might not be glossing over as much as we think as different filters
can specialize to captures different features, but then there is the problem of
during generation, if all else is equal, the network currently will have no way
of knowing for example that the thirds that are so favorable in the upper ranges
should be forbidden in the lower ranges.  There are ways to overcome this by
changing the architecture, for example by having locally connected layers where
different filters are assigned to different registers allowing them to learn
patterns specific to that pitch height, and how different pitch registers relate
to each other.  We are running experiments on such variations and one of the
next steps is also to understand what the filters are learning in practice.&lt;/p&gt;

&lt;p&gt;Our network is currently 28 layers deep.  To increase the training efficiency,
we use batch normalization to reduce internal covariate shift.  We also add skip
connections between layers as in ResNet introduced by He et al to enable
features from shallower layers to be reused directly in deeper layers.  We
haven’t done much hyperparameter tuning yet so the model could change a lot.
Stay tuned!&lt;/p&gt;

&lt;h3 id=&quot;representation-satb-like-rgb&quot;&gt;Representation: SATB like RGB&lt;/h3&gt;
&lt;p&gt;To address the challenge mentioned in the beginning of this blog of music
consisting of multiple simultaneous streams but often modeled as one, we
represent polyphonic music as a stack of piano rolls, $X$, of shape $(T, P, K)$
indicating time, pitch and instrument where each voice has its own $X^k$, akin
to how there are separate RGB channels in images.  This allows us to model the
interaction between voices, to capture both voice-leading and harmonic
progressions.  This separation of voices also yields another benefit, if we
assume that each voice is now monophonic, we can model predictions as softmaxes
across pitch.  Also for generation, we can easily condition on an incomplete
score that has voices missing for some timesteps.&lt;/p&gt;

&lt;p&gt;####Using masks to indicate which time steps to fill in&lt;/p&gt;

&lt;p&gt;So how does the model know which time steps on which instruments were blanked
out and needs to be restored?  Each time step of an instrument $x_t^k$ has an
accompanying binary mask $m_t^k$ that uses 1 to indicate “please figure out
which pitch should be played here” and 0 to indicate “nothing to be done there”.
Note each $m_t^k$ is actually a vector of 1s or 0s of size $P$ because if a time
step is blanked out we blank out the entire column in the piano roll.  Hence the
conditional distributions we are learning for predicting a particular blank out
is the following:&lt;/p&gt;

&lt;center&gt;$P(x_t^k | \mathbf{X} (1-\mathbf{M}), \mathbf{M})$&lt;/center&gt;

&lt;p&gt;####Training data: blanking out Bach chorales to teach the model counterpoint&lt;/p&gt;

&lt;p&gt;We’re starting with four-part Bach chorales.  We set up the training data by
cropping pieces of four measures, and for each crop we randomly blank out 4
measure-long patches on any of the SATB parts.  When these blank outs overlap in
time, there could be one to three voices present, and the model is forced to
learn how to write counterpoint in the presence of different amounts of
information.  For example, the soprano and the bass line could be intact, and
the model needs to figure out the inner voices.  When these blank outs overlap
on an instrument, a much longer time span is empty, and the model has to learn
how to sustain longer melodic arches.&lt;/p&gt;

&lt;p&gt;If we are very thoughtful in setting up the blank out procedure, this model can
potentially support a wide range of musical tasks such as harmonization,
interpolation and elaboration etc.  Harmonization is the task of given a melody,
generate an accompaniment of multiple voices or chords.  Interpolation is the
task of connecting or transitioning from one section to another.  I am using the
word section loosely here to mean a chunk of music of some length.  It could be
a few beats or it could be a phrase or actually a section.  One kind of
elaboration could be starting with the skeleton of a chord progression and then
the task is to expand the chords into full-fledged music of a certain style.&lt;/p&gt;

&lt;h3 id=&quot;todo-lets-see-coconet-in-action&quot;&gt;[TODO] Let’s see Coconet in action!&lt;/h3&gt;

&lt;p&gt;Let’s walk through an example to see how Coconet generates from scratch.  Here
we asked the model to generate two measures of music in the style of four-part
Bach chorales.  Starting with a blank score, the model adds notes one by one.
At every step, the model predicts for each cell in the piano roll, how likely is
it for this pitch to be part of the score given all the other notes that are
already on the score.  The plots below highlight some of the interesting turns
in the sequence of predictions. Each plot is a grayscale heatmap on a piano roll
showing its softmax predictive distribution for each timestep based on what was
already generated, shown in solid magenta colors.  The magenta box with cyan
border shows the most recently generated note, while the empty cyan box points
to the pitch that has just been sampled from the softmax.  On the heatmap
spectrum, the black end means the model is very confident given what was
composed so far there should be a note there, while white means absolutely not.
Grey can be very interesting because it reveals the model considering multiple
alternatives.&lt;/p&gt;

&lt;p&gt;Step 1: What are the chances the first sampled pitch is a C?&lt;/p&gt;

&lt;p&gt;Step 2: Given the high C, we see the adjacent timesteps become black because
notes don’t just pop in and out but usually stay for a certain duration.  Also
we see grey patches approaching this high-C from above and be fan into this
high-C while the model is wondering if we should approach this generated note
from above or below.  And the dice happened to choose the G from below.&lt;/p&gt;

&lt;p&gt;Step 16:
Step 17:&lt;/p&gt;

&lt;p&gt;Overall for pitch For pitch, the model happen to generate in the range of G4
(midi pitch 67) to E5 (midi pitch 88) which is a&lt;/p&gt;

&lt;p&gt;A series of plots here to show how the model starts with an empty canvas and
adds notes to it.
- multimodal, how it chooses between modes, what are the different musical
  implications of the different modes, are they close or far (look ahead)
- in what musical context is it high entropy, and which is it low, is there
  something we can learn beyond confirming basic music theory?&lt;/p&gt;

&lt;p&gt;Let’s look at what the model is predicting at each step. The advantage of this
model is that it always returns predictions over the entire piano roll, not just
the next time step.  This allows us to see a lot of structure in the prediction
and debug which musical relationships the model is doing well on and which it’s
missing.&lt;/p&gt;

&lt;p&gt;Let’s walk through an example of a short segment of counterpoint is generated
from scratch by the model.  That means we’re neither conditioning on some
existing piece nor seeding it with some initial music.&lt;/p&gt;

&lt;p&gt;[TODO: increase fontsize for title]&lt;/p&gt;

&lt;p&gt;storyboard-generating_from_scratch.png&lt;/p&gt;

&lt;h3 id=&quot;todo-hear-the-rewriting-getting-better&quot;&gt;[TODO] Hear the rewriting getting better?&lt;/h3&gt;

&lt;h2 id=&quot;how-should-we-generate--conditionally-sequentially-but-not-chronologically&quot;&gt;How should we generate?  Conditionally, sequentially, but not chronologically&lt;/h2&gt;

&lt;p&gt;Conditioned sequential generation is the procedure of generating one after
another, where at each step the generated is fed back into the network to
predict the next.  This allows the prediction of the current step to be
conditioned on previous predictions as oppose to making independent predictions.
This is how RNNs naturally generate, but this procedure is not limited to RNNs
because what it requires is that the conditionals can form a chain.  For example
&lt;a href=&quot;http://arxiv.org/abs/1609.03499&quot;&gt;WaveNet&lt;/a&gt; uses CNNs to model its forward conditionals, and generates waveforms
sample by sample in chronological order.  This way of sampling is also not
limited to objects that are inherently sequential.  For example,
&lt;a href=&quot;pixelcnn&quot;&gt;PixelCNN&lt;/a&gt; generates 2D images row by row and within each row pixel by
pixel, again using CNNs to model its conditionals.  In such a generative
process, a model becomes progressively more informed as the already-generated
parts helps determine what comes next.&lt;/p&gt;

&lt;p&gt;However, the early parts in the generation also constrains what could happen in
the later parts, and that means the ordering of generation has a large impact on
what can be generated.  For example, if we choose to generate chronologically,
the beginning of a sequence will have the most say over the arch of the phrase.
In our preliminary experiments, we see that chorales generated chronologically
favors step-wise motion and would generate for the melody, a long run of notes
up the scale.  This intuitively makes sense because if we start from the
beginning and not know where we’re going a safe move would be to move in small
steps and to continue doing that to maintain coherence.  This is the example
given in &lt;a href=&quot;#forwardSample&quot;&gt;the beginning of this blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As our model is not constrained to generate chronologically or to generate in a
fixed order, we experimented with generating notes within a voice in random
order.  The intuition is that this would allow the model to make some creative
moves in different parts of the piece and then in the context of this have the
other time steps make it work.  We did indeed see the model generating more
interesting chorales.  You can hear an example of a new chorale with notes in
voices generated at random order at &lt;a href=&quot;#randomSample&quot;&gt;the end of this blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although generating in random order yields more interesting results, it is only
revealing the tip of an iceberg.  An desirable ordering would be sensitive to
how entropy is distributed in a piece, how multi-model is current voice given
all the generated parts.  Perhaps we want to work out the most constrained parts
first and then see what freedom we have, or perhaps we can first sample the
parts with highest entropy and then work towards it.  Could we learn an
iterative construction process like DRAW for generating scores?  Could we first
generate the underlying structure of a piece analogous to a blurred image and
then iteratively refine the details?&lt;/p&gt;

&lt;p&gt;##Time to generate some new Bach chorales! &lt;a name=&quot;randomSample&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This excerpt was generated with a very similar procedure to the one in the
beginning of the blog.  It was generated by rewriting an existing Bach chorale
voice by voice, the difference is within a voice the notes are generated in
random order, as oppose to going from left to right.&lt;/p&gt;

&lt;!--We could blank out one of the voices in a Bach chorale, and ask our
inpainting model to generate that voice conditioned on the others.  We thought
we take that a step further by taking all the voices out one by one, so that at
the end we have a brand new piece. --&gt;

&lt;!--So more concretely the generation story goes like this:  we start with a 4-part Bach chorale, we blank out one voice, ask the model to fill it back in.  The model performs inpainting for this voice by generating its time steps in random order, where at each time step the generated is fed back into the model.  When we're done with this voice, we blank out another one, and fill it back in by conditioning now on two original parts from Bach and one from our model.  Do this until all voices are from our model. --&gt;

&lt;iframe width=&quot;300&quot; height=&quot;200&quot; src=&quot;https://www.youtube.com/embed/4P9R8QwvUVU&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For comparison, this is the forward generated excerpt from the beginning of this blog.&lt;/p&gt;
&lt;iframe width=&quot;300&quot; height=&quot;200&quot; src=&quot;https://www.youtube.com/embed/JBVGeZuE5gk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;###So was the model successful?
[TODO: comparison to forward generation, still coherent, a lot freer, even though it was generated out of order, it still resolves dissonances…]
The model seems to have mastered counterpoint quite well.  We’ll provide more analysis in our upcoming arXiv paper.  For now, the overall impression is that it’s able to work out voice leading, knows what harmonic intervals to prefer and how to resolve dissonances.  It keeps the music flowing, with some voices moving faster at times and others more sustained.  The melody carries an arch.  There’s nice surprises that makes me want to listen to it again to figure it out.  It knows the key and even concludes on a Picardy third at the end! You can imagine how excited we were when we heard that from our model!
&lt;!--Yes, that means the excerpt on the right was generated by our inpainting model.--&gt;&lt;/p&gt;

&lt;h2 id=&quot;todoconclusions&quot;&gt;[TODO]Conclusions&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;in what way better?&lt;/li&gt;
  &lt;li&gt;advantage of generating in a random order, future work, explore how to figure out what ordering is better&lt;/li&gt;
  &lt;li&gt;generation procedure of rewriting, allowing the model to condition on an existing pieces structure..abstract and long-term structure, such as chord progression and arch.
Our approach can be used for conditioned music generation such as completion of a partial score or variation on existing works. Unlike prior works in music generation, our approach is not constrained to producing notes in chronological order, and suffers much less from the accumulation of errors characteristic of chronological models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;todofuture-work&quot;&gt;[TODO]Future work&lt;/h2&gt;
&lt;p&gt;How well does it follow, how does it do the creative moves it does?
Understanding what features were learned…
Model longer term structure&lt;/p&gt;

&lt;p&gt;##Acknowledgement
Last by not least, this work was carried out with collaborators on the &lt;a href=&quot;magenta&quot;&gt;Magenta&lt;/a&gt; team in &lt;a href=&quot;brain&quot;&gt;Google Brain&lt;/a&gt;, including fellow intern &lt;a href=&quot;cooijmans&quot;&gt;Tim Cooijmans&lt;/a&gt;, my summer host &lt;a href=&quot;adam&quot;&gt;Adam Roberts&lt;/a&gt; and my super host &lt;a href=&quot;doug&quot;&gt;Douglas Eck&lt;/a&gt;.  Special thanks to Jason Freidenfelds for his insight during my practice talk for this work.&lt;/p&gt;

&lt;p&gt;Sign up for our &lt;a href=&quot;magenta-list&quot;&gt;mailing list&lt;/a&gt; to stayed tuned on upcoming arXiv paper.  Our models are implemented in &lt;a href=&quot;tensorflow&quot;&gt;Tensorflow&lt;/a&gt; and will released soon on Magenta’s &lt;a href=&quot;magenta-github&quot;&gt;github&lt;/a&gt;.  Thanks for listening!&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Oct 2016 15:00:00 +0000</pubDate>
        <link>http://localhost:4000/2016/10/11/coconet/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/10/11/coconet/</guid>
        
        <category>magenta</category>
        
        
      </item>
    
  </channel>
</rss>
